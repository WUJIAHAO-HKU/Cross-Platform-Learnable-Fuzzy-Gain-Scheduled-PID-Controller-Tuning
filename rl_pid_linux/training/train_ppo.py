"""
PPOè®­ç»ƒè„šæœ¬ï¼ˆæ›¿ä»£DDPGï¼‰
PPOå¯¹ä¸ç¨³å®šç¯å¢ƒæ›´é²æ£’
åŒ…å«å®Œæ•´çš„èµ„æºæ¸…ç†æœºåˆ¶ï¼Œé˜²æ­¢GPUæ³„æ¼
"""

import os
import argparse
import yaml
import torch
import signal
import atexit
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor

import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from envs.franka_env import FrankaRLPIDEnv

# å…¨å±€å˜é‡ç”¨äºèµ„æºæ¸…ç†
train_env_global = None
eval_env_global = None
model_global = None


def cleanup_resources():
    """æ¸…ç†GPUå’Œç¯å¢ƒèµ„æº"""
    global train_env_global, eval_env_global, model_global
    
    print("\nğŸ§¹ æ­£åœ¨æ¸…ç†èµ„æº...")
    
    try:
        # å…³é—­ç¯å¢ƒ
        if train_env_global is not None:
            try:
                train_env_global.close()
                print("   âœ… è®­ç»ƒç¯å¢ƒå·²å…³é—­")
            except Exception as e:
                print(f"   âš ï¸  è®­ç»ƒç¯å¢ƒå…³é—­å¤±è´¥: {e}")
        
        if eval_env_global is not None:
            try:
                eval_env_global.close()
                print("   âœ… è¯„ä¼°ç¯å¢ƒå·²å…³é—­")
            except Exception as e:
                print(f"   âš ï¸  è¯„ä¼°ç¯å¢ƒå…³é—­å¤±è´¥: {e}")
        
        # æ¸…ç†GPUç¼“å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("   âœ… GPUç¼“å­˜å·²æ¸…ç©º")
    
    except Exception as e:
        print(f"   âš ï¸  æ¸…ç†è¿‡ç¨‹å‡ºé”™: {e}")
    
    print("   âœ… èµ„æºæ¸…ç†å®Œæˆ")


def signal_handler(sig, frame):
    """å¤„ç†ä¸­æ–­ä¿¡å·ï¼ˆCtrl+Cï¼‰"""
    print("\nâš ï¸  æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å· (Ctrl+C)")
    cleanup_resources()
    print("   ç¨‹åºå·²å®‰å…¨é€€å‡º")
    sys.exit(0)


# æ³¨å†Œä¿¡å·å¤„ç†å™¨å’Œé€€å‡ºæ¸…ç†
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)
atexit.register(cleanup_resources)


def train(config_path, output_dir='./logs', model_name='rl_pid_ppo'):
    """è®­ç»ƒPPOæ¨¡å‹"""
    global train_env_global, eval_env_global, model_global
    
    # åŠ è½½é…ç½®
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # GPUæ£€æµ‹
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print("\n" + "="*70)
    print("  RL+PID PPOè®­ç»ƒ")
    print("="*70)
    print(f"\nğŸ–¥ï¸  è®¾å¤‡æ£€æµ‹:")
    print(f"   PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"   ä½¿ç”¨è®¾å¤‡: {device.upper()}")
    if device == 'cuda':
        print(f"   GPUåç§°: {torch.cuda.get_device_name(0)}")
        print(f"   GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        print("   âœ… GPUåŠ é€Ÿå·²å¯ç”¨ï¼")
    else:
        print("   âš ï¸  GPUä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè®­ç»ƒ")
    
    print(f"\nâœ… é…ç½®åŠ è½½å®Œæˆ: {config_path}")
    print(f"   Delta Scale Max: {config['rl_params']['delta_scale_max']}")
    print(f"   Total Timesteps: {config['training']['total_timesteps']}")
    
    # åˆ›å»ºç¯å¢ƒ
    print(f"\nâœ… åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    n_envs = config.get('n_envs', 4)  # PPOæ¨èå¤šç¯å¢ƒ
    
    # â­ ä½¿ç”¨çœŸæ­£çš„å¤šè¿›ç¨‹å¹¶è¡Œï¼ˆSubprocVecEnvï¼‰
    # å­è¿›ç¨‹è¿è¡Œç¯å¢ƒä»¿çœŸï¼ˆCPUï¼‰ï¼Œä¸»è¿›ç¨‹è¿è¡Œç¥ç»ç½‘ç»œï¼ˆGPUï¼‰
    if n_envs > 1:
        print(f"   ä½¿ç”¨ {n_envs} ä¸ªå¹¶è¡Œè¿›ç¨‹ (SubprocVecEnv)")
        if device == 'cuda':
            print(f"   âš™ï¸  ç¯å¢ƒä»¿çœŸåœ¨å­è¿›ç¨‹(CPU) + ç¥ç»ç½‘ç»œåœ¨ä¸»è¿›ç¨‹(GPU)")
        else:
            print(f"   âš™ï¸  å…¨éƒ¨ä½¿ç”¨CPUæ¨¡å¼")
        
        def make_env(rank):
            """
            åˆ›å»ºç¯å¢ƒçš„å·¥å‚å‡½æ•°
            æ¯ä¸ªå­è¿›ç¨‹ä¼šç‹¬ç«‹è°ƒç”¨è¿™ä¸ªå‡½æ•°
            """
            def _init():
                # åœ¨å­è¿›ç¨‹ä¸­åˆ›å»ºç¯å¢ƒï¼ˆä¸ä½¿ç”¨GPUï¼‰
                env = FrankaRLPIDEnv(config, gui=False)
                return Monitor(env, info_keywords=())
            return _init
        
        # åˆ›å»ºå¤šä¸ªç‹¬ç«‹çš„å­è¿›ç¨‹ï¼Œæ¯ä¸ªè¿è¡Œä¸€ä¸ªç¯å¢ƒ
        train_env = SubprocVecEnv([make_env(i) for i in range(n_envs)])
        print(f"   âœ… {n_envs} ä¸ªå¹¶è¡Œè®­ç»ƒç¯å¢ƒåˆ›å»ºæˆåŠŸ")
    else:
        print("   ä½¿ç”¨å•ç¯å¢ƒè®­ç»ƒæ¨¡å¼")
        train_env = FrankaRLPIDEnv(config, gui=False)
        train_env = Monitor(train_env)
        print("   âœ… å•ç¯å¢ƒè®­ç»ƒæ¨¡å¼")
    
    # è¯„ä¼°ç¯å¢ƒ
    eval_env = FrankaRLPIDEnv(config, gui=False)
    eval_env = Monitor(eval_env)
    
    # â­ ä¿å­˜åˆ°å…¨å±€å˜é‡ä»¥ä¾¿æ¸…ç†
    train_env_global = train_env
    eval_env_global = eval_env
    
    # PPOè¶…å‚æ•°
    rl_params = config.get('rl_params', {})
    training_config = config.get('training', {})
    
    # â­ PPOç‰¹æœ‰å‚æ•°
    n_steps = 2048  # æ¯æ¬¡æ›´æ–°çš„æ­¥æ•°ï¼ˆ4ä¸ªç¯å¢ƒÃ—2048=8192æ ·æœ¬/æ›´æ–°ï¼‰
    batch_size = 256  # mini-batchå¤§å°ï¼ˆ8192/256=32ä¸ªmini-batchï¼‰
    n_epochs = 10    # æ¯æ¬¡æ›´æ–°çš„epochæ•°
    
    print(f"\nâœ… åˆ›å»ºPPOæ¨¡å‹...")
    print(f"   n_steps: {n_steps}")
    print(f"   batch_size: {batch_size}")
    print(f"   n_epochs: {n_epochs}")
    
    model = PPO(
        "MlpPolicy",
        train_env,
        learning_rate=rl_params.get('learning_rate_actor', 0.0003),
        n_steps=n_steps,
        batch_size=batch_size,
        n_epochs=n_epochs,
        gamma=rl_params.get('gamma', 0.99),
        gae_lambda=0.95,  # GAEå‚æ•°
        clip_range=0.2,   # PPOè£å‰ªå‚æ•°
        ent_coef=0.01,    # ç†µç³»æ•°ï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰
        vf_coef=0.5,      # ä»·å€¼å‡½æ•°ç³»æ•°
        max_grad_norm=0.5,
        policy_kwargs=dict(
            net_arch=dict(
                pi=rl_params.get('actor_hidden', [256, 128, 64]),
                vf=rl_params.get('critic_hidden', [256, 256, 128])
            )
        ),
        tensorboard_log=f"{output_dir}/tensorboard/",
        device=device,
        verbose=1
    )
    
    print("   âœ… PPOæ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # â­ ä¿å­˜æ¨¡å‹åˆ°å…¨å±€å˜é‡
    model_global = model
    
    # Callbacks
    os.makedirs(output_dir, exist_ok=True)
    
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=f"{output_dir}/best_model/",
        log_path=f"{output_dir}/eval/",
        eval_freq=training_config.get('eval_freq', 10000),
        n_eval_episodes=training_config.get('n_eval_episodes', 5),
        deterministic=True,
        render=False
    )
    
    checkpoint_callback = CheckpointCallback(
        save_freq=training_config.get('save_freq', 50000),
        save_path=f"{output_dir}/checkpoints/",
        name_prefix=model_name
    )
    
    # è®­ç»ƒ
    print("\n" + "="*70)
    print("  å¼€å§‹è®­ç»ƒ")
    print("="*70)
    print(f"\nğŸ“Š ç›‘æ§è®­ç»ƒè¿›åº¦ï¼š")
    print(f"   tensorboard --logdir={output_dir}/tensorboard/\n")
    
    try:
        model.learn(
            total_timesteps=training_config.get('total_timesteps', 500000),
            callback=[eval_callback, checkpoint_callback],
            log_interval=training_config.get('log_interval', 10),
            progress_bar=False  # â­ ç¦ç”¨è¿›åº¦æ¡é¿å…ä¾èµ–é—®é¢˜
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = f"{output_dir}/{model_name}_final"
        model.save(final_model_path)
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜è‡³: {final_model_path}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ä¸­æ–­ (Ctrl+C)")
        try:
            interrupted_path = f"{output_dir}/{model_name}_interrupted"
            model.save(interrupted_path)
            print(f"   âœ… ä¸­æ–­æ—¶çš„æ¨¡å‹å·²ä¿å­˜è‡³: {interrupted_path}")
        except Exception as e:
            print(f"   âš ï¸  ä¿å­˜ä¸­æ–­æ¨¡å‹å¤±è´¥: {e}")
    
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹å‡ºé”™: {e}")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        try:
            error_path = f"{output_dir}/{model_name}_error"
            model.save(error_path)
            print(f"   âœ… é”™è¯¯æ—¶çš„æ¨¡å‹å·²ä¿å­˜è‡³: {error_path}")
        except Exception as save_error:
            print(f"   âš ï¸  ä¿å­˜é”™è¯¯æ¨¡å‹å¤±è´¥: {save_error}")
    
    finally:
        # â­ ç¡®ä¿èµ„æºä¸€å®šè¢«æ¸…ç†
        print("\nğŸ§¹ æ¸…ç†è®­ç»ƒèµ„æº...")
        try:
            train_env.close()
            print("   âœ… è®­ç»ƒç¯å¢ƒå·²å…³é—­")
        except Exception as e:
            print(f"   âš ï¸  å…³é—­è®­ç»ƒç¯å¢ƒå¤±è´¥: {e}")
        
        try:
            eval_env.close()
            print("   âœ… è¯„ä¼°ç¯å¢ƒå·²å…³é—­")
        except Exception as e:
            print(f"   âš ï¸  å…³é—­è¯„ä¼°ç¯å¢ƒå¤±è´¥: {e}")
        
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("   âœ… GPUç¼“å­˜å·²æ¸…ç©º")
        
        print("   âœ… æ‰€æœ‰èµ„æºå·²æ¸…ç†å®Œæˆ")


def main():
    parser = argparse.ArgumentParser(description='PPOè®­ç»ƒ')
    parser.add_argument('--config', type=str, required=True,
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='./logs',
                        help='è¾“å‡ºç›®å½•')
    parser.add_argument('--name', type=str, default='rl_pid_ppo',
                        help='æ¨¡å‹åç§°')
    
    args = parser.parse_args()
    train(args.config, args.output, args.name)


if __name__ == '__main__':
    main()

