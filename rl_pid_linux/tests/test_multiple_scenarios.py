"""
å¤šåœºæ™¯æµ‹è¯•è„šæœ¬ï¼šå¯¹æ¯”çº¯PID vs RL+PIDåœ¨ä¸åŒè½¨è¿¹ä¸‹çš„æ€§èƒ½

æµ‹è¯•åœºæ™¯ï¼š
1. æ…¢é€Ÿåœ†å½¢ï¼ˆå·²çŸ¥RLæ”¹è¿›1.71%ï¼‰
2. å¿«é€Ÿåœ†å½¢
3. æ­£å¼¦è½¨è¿¹
4. å¿«é€Ÿæ­£å¼¦
5. é˜¶è·ƒè½¨è¿¹
6. 8å­—å½¢è½¨è¿¹ï¼ˆå¤æ‚è½¨è¿¹ï¼‰
"""

import yaml
import numpy as np
import argparse
import json
from stable_baselines3 import PPO
from envs.franka_env import FrankaRLPIDEnv
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from datetime import datetime
import os

# å®šä¹‰æµ‹è¯•åœºæ™¯
SCENARIOS = [
    {
        'name': 'æ…¢é€Ÿåœ†å½¢',
        'type': 'circle',
        'params': {'speed': 0.2, 'radius': 0.15},
        'difficulty': 'ç®€å•',
        'description': 'åŸºå‡†åœºæ™¯ï¼ŒPIDåº”è¯¥è¡¨ç°è‰¯å¥½'
    },
    {
        'name': 'å¿«é€Ÿåœ†å½¢',
        'type': 'circle',
        'params': {'speed': 0.5, 'radius': 0.15},
        'difficulty': 'ä¸­ç­‰',
        'description': 'é«˜é€Ÿè¿åŠ¨ï¼ŒPIDå¯èƒ½å“åº”ä¸å¤Ÿå¿«'
    },
    {
        'name': 'æ­£å¼¦è½¨è¿¹',
        'type': 'sine',
        'params': {'frequency': 0.3, 'amplitude': 0.5},
        'difficulty': 'ä¸­ç­‰',
        'description': 'å‘¨æœŸæ€§å˜åŒ–ï¼Œæµ‹è¯•è·Ÿè¸ªèƒ½åŠ›'
    },
    {
        'name': 'å¿«é€Ÿæ­£å¼¦',
        'type': 'sine',
        'params': {'frequency': 0.8, 'amplitude': 0.5},
        'difficulty': 'å›°éš¾',
        'description': 'é«˜é¢‘ä¿¡å·ï¼ŒPIDå¾®åˆ†é¡¹å¯èƒ½å¼•å…¥å™ªå£°'
    },
    {
        'name': 'é˜¶è·ƒè½¨è¿¹',
        'type': 'step',
        'params': {'interval': 2.0, 'amplitude': 0.3},
        'difficulty': 'å›°éš¾',
        'description': 'ä¸è¿ç»­è½¨è¿¹ï¼ŒåŠ é€Ÿåº¦çªå˜'
    },
    {
        'name': 'é™æ€ä¿æŒ',
        'type': 'static',
        'params': {},
        'difficulty': 'ç®€å•',
        'description': 'é™æ€ä¿æŒï¼Œæµ‹è¯•ç¨³æ€æ€§èƒ½'
    }
]


def run_single_test(env, model, scenario, num_steps=10000, use_rl=False):
    """
    è¿è¡Œå•æ¬¡æµ‹è¯•
    
    Args:
        env: ç¯å¢ƒ
        model: RLæ¨¡å‹ï¼ˆå¦‚æœuse_rl=Trueï¼‰
        scenario: åœºæ™¯é…ç½®
        num_steps: æµ‹è¯•æ­¥æ•°
        use_rl: æ˜¯å¦ä½¿ç”¨RLç­–ç•¥
    
    Returns:
        dict: æµ‹è¯•ç»“æœ
    """
    obs, _ = env.reset()
    
    errors = []
    rewards = []
    actions = []
    delta_taus = []
    
    for step in range(num_steps):
        if use_rl:
            # ä½¿ç”¨RL+PID
            action, _ = model.predict(obs, deterministic=True)
            actions.append(action)
        else:
            # çº¯PIDï¼ˆaction=0ï¼‰
            action = np.zeros(env.action_space.shape)
        
        obs, reward, done, truncated, info = env.step(action)
        
        # è®°å½•æ•°æ®
        err_norm = np.linalg.norm(info['tracking_error'])
        errors.append(err_norm)
        rewards.append(reward)
        
        if use_rl and 'delta_tau' in info:
            delta_taus.append(np.linalg.norm(info['delta_tau']))
        
        if done or truncated:
            break
    
    # è®¡ç®—ç»Ÿè®¡é‡
    errors = np.array(errors)
    rewards = np.array(rewards)
    
    results = {
        'mean_error': float(np.mean(errors)),
        'median_error': float(np.median(errors)),
        'max_error': float(np.max(errors)),
        'std_error': float(np.std(errors)),
        'total_reward': float(np.sum(rewards)),
        'mean_reward': float(np.mean(rewards)),
        'error_history': errors.tolist()
    }
    
    if use_rl and delta_taus:
        delta_taus = np.array(delta_taus)
        results['mean_delta_tau'] = float(np.mean(delta_taus))
        results['max_delta_tau'] = float(np.max(delta_taus))
        results['mean_action_norm'] = float(np.mean([np.linalg.norm(a) for a in actions]))
    
    return results


def test_all_scenarios(model_path, config_path, num_repeats=3, num_steps=10000):
    """
    æµ‹è¯•æ‰€æœ‰åœºæ™¯
    
    Args:
        model_path: RLæ¨¡å‹è·¯å¾„
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        num_repeats: æ¯ä¸ªåœºæ™¯é‡å¤æ¬¡æ•°
        num_steps: æ¯æ¬¡æµ‹è¯•æ­¥æ•°
    
    Returns:
        dict: æ‰€æœ‰æµ‹è¯•ç»“æœ
    """
    # åŠ è½½é…ç½®
    with open(config_path, 'r') as f:
        base_config = yaml.safe_load(f)
    
    # åŠ è½½RLæ¨¡å‹
    model = PPO.load(model_path.replace('.zip', ''))
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {model_path}")
    
    all_results = {}
    
    for scenario in SCENARIOS:
        print("\n" + "=" * 70)
        print(f"æµ‹è¯•åœºæ™¯: {scenario['name']} ({scenario['difficulty']})")
        print(f"æè¿°: {scenario['description']}")
        print("=" * 70)
        
        # ä¿®æ”¹é…ç½®
        test_config = base_config.copy()
        test_config['trajectory']['type'] = scenario['type']
        test_config['trajectory'].update(scenario['params'])
        
        scenario_results = {
            'name': scenario['name'],
            'type': scenario['type'],
            'difficulty': scenario['difficulty'],
            'description': scenario['description'],
            'params': scenario['params'],
            'pid_results': [],
            'rl_results': []
        }
        
        for repeat in range(num_repeats):
            print(f"\n  é‡å¤ {repeat+1}/{num_repeats}...")
            
            # æµ‹è¯•çº¯PID
            print("    [1/2] çº¯PIDæµ‹è¯•ä¸­...")
            env = FrankaRLPIDEnv(test_config, gui=False)
            pid_result = run_single_test(env, None, scenario, num_steps, use_rl=False)
            env.close()
            scenario_results['pid_results'].append(pid_result)
            print(f"          å¹³å‡è¯¯å·®: {pid_result['mean_error']:.4f}å¼§åº¦ ({np.rad2deg(pid_result['mean_error']):.2f}åº¦)")
            
            # æµ‹è¯•RL+PID
            print("    [2/2] RL+PIDæµ‹è¯•ä¸­...")
            env = FrankaRLPIDEnv(test_config, gui=False)
            rl_result = run_single_test(env, model, scenario, num_steps, use_rl=True)
            env.close()
            scenario_results['rl_results'].append(rl_result)
            print(f"          å¹³å‡è¯¯å·®: {rl_result['mean_error']:.4f}å¼§åº¦ ({np.rad2deg(rl_result['mean_error']):.2f}åº¦)")
        
        # è®¡ç®—å¹³å‡æ€§èƒ½
        pid_mean_error = np.mean([r['mean_error'] for r in scenario_results['pid_results']])
        rl_mean_error = np.mean([r['mean_error'] for r in scenario_results['rl_results']])
        improvement = (pid_mean_error - rl_mean_error) / pid_mean_error * 100
        
        scenario_results['summary'] = {
            'pid_mean_error': float(pid_mean_error),
            'pid_mean_error_deg': float(np.rad2deg(pid_mean_error)),
            'rl_mean_error': float(rl_mean_error),
            'rl_mean_error_deg': float(np.rad2deg(rl_mean_error)),
            'improvement_percent': float(improvement),
            'rl_mean_delta_tau': float(np.mean([r.get('mean_delta_tau', 0) for r in scenario_results['rl_results']]))
        }
        
        print(f"\n  ğŸ“Š åœºæ™¯æ€»ç»“:")
        print(f"     çº¯PID:  {pid_mean_error:.4f}å¼§åº¦ ({np.rad2deg(pid_mean_error):.2f}åº¦)")
        print(f"     RL+PID: {rl_mean_error:.4f}å¼§åº¦ ({np.rad2deg(rl_mean_error):.2f}åº¦)")
        print(f"     æ”¹è¿›ç‡: {improvement:+.2f}%")
        
        all_results[scenario['name']] = scenario_results
    
    return all_results


def generate_comparison_plots(results, output_dir='results/multi_scenario'):
    """
    ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
    
    Args:
        results: æµ‹è¯•ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
    """
    os.makedirs(output_dir, exist_ok=True)
    
    # æå–æ•°æ®
    scenarios = list(results.keys())
    pid_errors = [results[s]['summary']['pid_mean_error_deg'] for s in scenarios]
    rl_errors = [results[s]['summary']['rl_mean_error_deg'] for s in scenarios]
    improvements = [results[s]['summary']['improvement_percent'] for s in scenarios]
    difficulties = [results[s]['difficulty'] for s in scenarios]
    
    # åˆ›å»ºå›¾è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # 1. è¯¯å·®å¯¹æ¯”æŸ±çŠ¶å›¾
    ax1 = axes[0, 0]
    x = np.arange(len(scenarios))
    width = 0.35
    
    bars1 = ax1.bar(x - width/2, pid_errors, width, label='Pure PID', color='steelblue', alpha=0.8)
    bars2 = ax1.bar(x + width/2, rl_errors, width, label='RL+PID', color='coral', alpha=0.8)
    
    ax1.set_xlabel('Trajectory Type', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Mean Tracking Error (degrees)', fontsize=12, fontweight='bold')
    ax1.set_title('Tracking Error Comparison Across Scenarios', fontsize=14, fontweight='bold')
    ax1.set_xticks(x)
    ax1.set_xticklabels(scenarios, rotation=15, ha='right')
    ax1.legend(fontsize=11)
    ax1.grid(axis='y', alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{height:.1f}Â°', ha='center', va='bottom', fontsize=9)
    
    # 2. æ”¹è¿›ç‡æŸ±çŠ¶å›¾
    ax2 = axes[0, 1]
    colors = ['green' if imp > 10 else 'orange' if imp > 5 else 'lightcoral' for imp in improvements]
    bars = ax2.bar(scenarios, improvements, color=colors, alpha=0.8, edgecolor='black')
    
    ax2.set_xlabel('Trajectory Type', fontsize=12, fontweight='bold')
    ax2.set_ylabel('Improvement (%)', fontsize=12, fontweight='bold')
    ax2.set_title('RL Improvement Rate', fontsize=14, fontweight='bold')
    ax2.set_xticklabels(scenarios, rotation=15, ha='right')
    ax2.axhline(y=0, color='black', linestyle='--', linewidth=1)
    ax2.grid(axis='y', alpha=0.3)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, imp in zip(bars, improvements):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{imp:+.1f}%', ha='center', va='bottom' if imp > 0 else 'top', fontsize=10)
    
    # 3. è¯¯å·®åˆ†å¸ƒç®±çº¿å›¾
    ax3 = axes[1, 0]
    pid_data = [results[s]['pid_results'][0]['error_history'] for s in scenarios]
    rl_data = [results[s]['rl_results'][0]['error_history'] for s in scenarios]
    
    # è½¬æ¢ä¸ºåº¦æ•°
    pid_data_deg = [np.rad2deg(d) for d in pid_data]
    rl_data_deg = [np.rad2deg(d) for d in rl_data]
    
    bp1 = ax3.boxplot(pid_data_deg, positions=np.arange(len(scenarios)) * 2 - 0.4,
                      widths=0.6, patch_artist=True,
                      boxprops=dict(facecolor='steelblue', alpha=0.6),
                      medianprops=dict(color='darkblue', linewidth=2))
    bp2 = ax3.boxplot(rl_data_deg, positions=np.arange(len(scenarios)) * 2 + 0.4,
                      widths=0.6, patch_artist=True,
                      boxprops=dict(facecolor='coral', alpha=0.6),
                      medianprops=dict(color='darkred', linewidth=2))
    
    ax3.set_xlabel('Trajectory Type', fontsize=12, fontweight='bold')
    ax3.set_ylabel('Tracking Error Distribution (degrees)', fontsize=12, fontweight='bold')
    ax3.set_title('Error Distribution Comparison', fontsize=14, fontweight='bold')
    ax3.set_xticks(np.arange(len(scenarios)) * 2)
    ax3.set_xticklabels(scenarios, rotation=15, ha='right')
    ax3.legend([bp1["boxes"][0], bp2["boxes"][0]], ['PID', 'RL+PID'], loc='upper right')
    ax3.grid(axis='y', alpha=0.3)
    
    # 4. éš¾åº¦vsæ”¹è¿›ç‡æ•£ç‚¹å›¾
    ax4 = axes[1, 1]
    difficulty_map = {'ç®€å•': 1, 'ä¸­ç­‰': 2, 'å›°éš¾': 3}
    difficulty_values = [difficulty_map[d] for d in difficulties]
    
    scatter = ax4.scatter(difficulty_values, improvements, c=improvements, 
                         cmap='RdYlGn', s=200, alpha=0.7, edgecolors='black', linewidth=2)
    
    for i, txt in enumerate(scenarios):
        ax4.annotate(txt, (difficulty_values[i], improvements[i]), 
                    xytext=(5, 5), textcoords='offset points', fontsize=9)
    
    ax4.set_xlabel('Scenario Difficulty', fontsize=12, fontweight='bold')
    ax4.set_ylabel('RL Improvement (%)', fontsize=12, fontweight='bold')
    ax4.set_title('Difficulty vs Improvement Correlation', fontsize=14, fontweight='bold')
    ax4.set_xticks([1, 2, 3])
    ax4.set_xticklabels(['Easy', 'Medium', 'Hard'])
    ax4.axhline(y=0, color='black', linestyle='--', linewidth=1)
    ax4.grid(True, alpha=0.3)
    
    plt.colorbar(scatter, ax=ax4, label='Improvement (%)')
    
    plt.tight_layout()
    plt.savefig(f'{output_dir}/multi_scenario_comparison.png', dpi=150, bbox_inches='tight')
    print(f"\nâœ… å¯¹æ¯”å›¾å·²ä¿å­˜: {output_dir}/multi_scenario_comparison.png")
    
    return fig


def generate_report(results, output_dir='results/multi_scenario'):
    """
    ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
    
    Args:
        results: æµ‹è¯•ç»“æœ
        output_dir: è¾“å‡ºç›®å½•
    """
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    report_path = f'{output_dir}/test_report_{timestamp}.txt'
    
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("å¤šåœºæ™¯æ€§èƒ½å¯¹æ¯”æµ‹è¯•æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # æ€»ä½“æ€»ç»“
        f.write("ã€æ€»ä½“æ€»ç»“ã€‘\n")
        f.write("-" * 80 + "\n")
        
        all_improvements = [results[s]['summary']['improvement_percent'] for s in results]
        avg_improvement = np.mean(all_improvements)
        max_improvement_scenario = max(results.keys(), key=lambda s: results[s]['summary']['improvement_percent'])
        min_improvement_scenario = min(results.keys(), key=lambda s: results[s]['summary']['improvement_percent'])
        
        f.write(f"æ€»æµ‹è¯•åœºæ™¯æ•°: {len(results)}\n")
        f.write(f"å¹³å‡æ”¹è¿›ç‡: {avg_improvement:.2f}%\n")
        f.write(f"æœ€ä½³æ”¹è¿›åœºæ™¯: {max_improvement_scenario} ({results[max_improvement_scenario]['summary']['improvement_percent']:.2f}%)\n")
        f.write(f"æœ€å·®æ”¹è¿›åœºæ™¯: {min_improvement_scenario} ({results[min_improvement_scenario]['summary']['improvement_percent']:.2f}%)\n\n")
        
        # å„åœºæ™¯è¯¦ç»†ç»“æœ
        f.write("ã€å„åœºæ™¯è¯¦ç»†ç»“æœã€‘\n")
        f.write("=" * 80 + "\n\n")
        
        for scenario_name, data in results.items():
            f.write(f"åœºæ™¯: {scenario_name}\n")
            f.write(f"éš¾åº¦: {data['difficulty']}\n")
            f.write(f"æè¿°: {data['description']}\n")
            f.write(f"è½¨è¿¹ç±»å‹: {data['type']}\n")
            f.write(f"å‚æ•°: {data['params']}\n")
            f.write("-" * 80 + "\n")
            
            summary = data['summary']
            f.write(f"çº¯PIDå¹³å‡è¯¯å·®:  {summary['pid_mean_error']:.4f}å¼§åº¦ ({summary['pid_mean_error_deg']:.2f}åº¦)\n")
            f.write(f"RL+PIDå¹³å‡è¯¯å·®: {summary['rl_mean_error']:.4f}å¼§åº¦ ({summary['rl_mean_error_deg']:.2f}åº¦)\n")
            f.write(f"æ”¹è¿›ç‡: {summary['improvement_percent']:+.2f}%\n")
            f.write(f"RLå¹³å‡è¡¥å¿åŠ›çŸ©: {summary['rl_mean_delta_tau']:.3f} Nm\n")
            f.write("\n")
        
        # ç»“è®ºä¸å»ºè®®
        f.write("ã€ç»“è®ºä¸å»ºè®®ã€‘\n")
        f.write("=" * 80 + "\n")
        
        # åˆ†æå“ªäº›åœºæ™¯RLæœ‰æ˜¾è‘—ä¼˜åŠ¿
        good_scenarios = [s for s in results if results[s]['summary']['improvement_percent'] > 10]
        medium_scenarios = [s for s in results if 5 < results[s]['summary']['improvement_percent'] <= 10]
        poor_scenarios = [s for s in results if results[s]['summary']['improvement_percent'] <= 5]
        
        f.write(f"\nRLæ˜¾è‘—ä¼˜åŠ¿åœºæ™¯ (æ”¹è¿›>10%): {len(good_scenarios)}ä¸ª\n")
        for s in good_scenarios:
            f.write(f"  - {s}: {results[s]['summary']['improvement_percent']:.2f}%\n")
        
        f.write(f"\nRLä¸­ç­‰ä¼˜åŠ¿åœºæ™¯ (5-10%): {len(medium_scenarios)}ä¸ª\n")
        for s in medium_scenarios:
            f.write(f"  - {s}: {results[s]['summary']['improvement_percent']:.2f}%\n")
        
        f.write(f"\nRLè½»å¾®ä¼˜åŠ¿åœºæ™¯ (<5%): {len(poor_scenarios)}ä¸ª\n")
        for s in poor_scenarios:
            f.write(f"  - {s}: {results[s]['summary']['improvement_percent']:.2f}%\n")
        
        f.write("\nå»ºè®®:\n")
        if len(good_scenarios) >= 2:
            f.write("âœ… RLåœ¨å¤šä¸ªå›°éš¾åœºæ™¯ä¸‹è¡¨ç°å‡ºæ˜¾è‘—ä¼˜åŠ¿ï¼Œå€¼å¾—å®é™…åº”ç”¨\n")
        elif len(medium_scenarios) >= 3:
            f.write("âš ï¸  RLåœ¨å¤§éƒ¨åˆ†åœºæ™¯ä¸‹æœ‰ä¸­ç­‰æ”¹è¿›ï¼Œå¯è€ƒè™‘åœ¨ç‰¹å®šåœºæ™¯ä½¿ç”¨\n")
        else:
            f.write("âŒ RLæ•´ä½“æ”¹è¿›æœ‰é™ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è®­ç»ƒç­–ç•¥æˆ–æ¥å—PIDå·²è¶³å¤Ÿå¥½çš„ç»“è®º\n")
    
    print(f"âœ… æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    return report_path


def main():
    parser = argparse.ArgumentParser(description='å¤šåœºæ™¯æ€§èƒ½å¯¹æ¯”æµ‹è¯•')
    parser.add_argument('--model', type=str, default='logs/best_model/best_model.zip',
                       help='RLæ¨¡å‹è·¯å¾„')
    parser.add_argument('--config', type=str, default='configs/stage1_optimized.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--repeats', type=int, default=3,
                       help='æ¯ä¸ªåœºæ™¯é‡å¤æ¬¡æ•°')
    parser.add_argument('--steps', type=int, default=10000,
                       help='æ¯æ¬¡æµ‹è¯•æ­¥æ•°')
    parser.add_argument('--output', type=str, default='results/multi_scenario',
                       help='è¾“å‡ºç›®å½•')
    args = parser.parse_args()
    
    print("=" * 80)
    print("å¤šåœºæ™¯æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 80)
    print(f"RLæ¨¡å‹: {args.model}")
    print(f"é…ç½®æ–‡ä»¶: {args.config}")
    print(f"é‡å¤æ¬¡æ•°: {args.repeats}")
    print(f"æµ‹è¯•æ­¥æ•°: {args.steps}")
    print(f"æµ‹è¯•åœºæ™¯æ•°: {len(SCENARIOS)}")
    print(f"æ€»æµ‹è¯•æ¬¡æ•°: {len(SCENARIOS) * args.repeats * 2} (PID + RL)")
    print("=" * 80)
    
    # è¿è¡Œæµ‹è¯•
    results = test_all_scenarios(args.model, args.config, args.repeats, args.steps)
    
    # ä¿å­˜åŸå§‹æ•°æ®
    os.makedirs(args.output, exist_ok=True)
    json_path = f'{args.output}/raw_results_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(json_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nâœ… åŸå§‹æ•°æ®å·²ä¿å­˜: {json_path}")
    
    # ç”Ÿæˆå›¾è¡¨
    generate_comparison_plots(results, args.output)
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_report(results, args.output)
    
    print("\n" + "=" * 80)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)


if __name__ == '__main__':
    main()

