"""
æµ‹è¯•é‡åŠ›è¡¥å¿æ•ˆæœ
å¯¹æ¯”æœ‰æ— é‡åŠ›è¡¥å¿çš„PIDæ€§èƒ½
"""

import yaml
import numpy as np
from envs.franka_env import FrankaRLPIDEnv
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

print("=" * 70)
print("ğŸ§ª é‡åŠ›è¡¥å¿æ•ˆæœæµ‹è¯•")
print("=" * 70)

# åŠ è½½é…ç½®
with open('configs/stage1_optimized.yaml', 'r') as f:
    config = yaml.safe_load(f)

# ================================================================================
# æµ‹è¯•1ï¼šæ— é‡åŠ›è¡¥å¿çš„PID
# ================================================================================
print("\n" + "=" * 70)
print("æµ‹è¯•1ï¼šPIDæ§åˆ¶ï¼ˆæ— é‡åŠ›è¡¥å¿ï¼‰")
print("=" * 70)

config_no_grav = config.copy()
config_no_grav['pid_params'] = config['pid_params'].copy()
config_no_grav['pid_params']['enable_gravity_compensation'] = False

env_no_grav = FrankaRLPIDEnv(config_no_grav, gui=False)
obs, _ = env_no_grav.reset()

errors_no_grav = []
rewards_no_grav = []
total_reward_no_grav = 0

for step in range(1000):
    action = np.zeros(7, dtype=np.float32)
    obs, reward, terminated, truncated, info = env_no_grav.step(action)
    errors_no_grav.append(info['err_norm'])
    rewards_no_grav.append(reward)
    total_reward_no_grav += reward
    
    if step % 200 == 0:
        print(f"  Step {step:4d}: err={info['err_norm']:.4f}, reward={reward:6.2f}")

print(f"\næ— é‡åŠ›è¡¥å¿PID:")
print(f"  å¹³å‡è¯¯å·®: {np.mean(errors_no_grav):.4f}å¼§åº¦ ({np.mean(errors_no_grav)*57.3:.2f}åº¦)")
print(f"  ä¸­ä½è¯¯å·®: {np.median(errors_no_grav):.4f}å¼§åº¦")
print(f"  æœ€å¤§è¯¯å·®: {np.max(errors_no_grav):.4f}å¼§åº¦")
print(f"  æœ€å°è¯¯å·®: {np.min(errors_no_grav):.4f}å¼§åº¦")
print(f"  æ€»å¥–åŠ±: {total_reward_no_grav:.1f}")

env_no_grav.close()

# ================================================================================
# æµ‹è¯•2ï¼šæœ‰é‡åŠ›è¡¥å¿çš„PID
# ================================================================================
print("\n" + "=" * 70)
print("æµ‹è¯•2ï¼šPIDæ§åˆ¶ï¼ˆæœ‰é‡åŠ›è¡¥å¿ï¼‰")
print("=" * 70)

config_with_grav = config.copy()
config_with_grav['pid_params'] = config['pid_params'].copy()
config_with_grav['pid_params']['enable_gravity_compensation'] = True

env_with_grav = FrankaRLPIDEnv(config_with_grav, gui=False)
obs, _ = env_with_grav.reset()

errors_with_grav = []
rewards_with_grav = []
total_reward_with_grav = 0

for step in range(1000):
    action = np.zeros(7, dtype=np.float32)
    obs, reward, terminated, truncated, info = env_with_grav.step(action)
    errors_with_grav.append(info['err_norm'])
    rewards_with_grav.append(reward)
    total_reward_with_grav += reward
    
    if step % 200 == 0:
        print(f"  Step {step:4d}: err={info['err_norm']:.4f}, reward={reward:6.2f}")

print(f"\næœ‰é‡åŠ›è¡¥å¿PID:")
print(f"  å¹³å‡è¯¯å·®: {np.mean(errors_with_grav):.4f}å¼§åº¦ ({np.mean(errors_with_grav)*57.3:.2f}åº¦)")
print(f"  ä¸­ä½è¯¯å·®: {np.median(errors_with_grav):.4f}å¼§åº¦")
print(f"  æœ€å¤§è¯¯å·®: {np.max(errors_with_grav):.4f}å¼§åº¦")
print(f"  æœ€å°è¯¯å·®: {np.min(errors_with_grav):.4f}å¼§åº¦")
print(f"  æ€»å¥–åŠ±: {total_reward_with_grav:.1f}")

env_with_grav.close()

# ================================================================================
# å¯¹æ¯”ç»“æœ
# ================================================================================
print("\n" + "=" * 70)
print("ğŸ“Š å¯¹æ¯”ç»“æœ")
print("=" * 70)

error_reduction = np.mean(errors_no_grav) - np.mean(errors_with_grav)
error_reduction_pct = (error_reduction / np.mean(errors_no_grav)) * 100
reward_improvement = total_reward_with_grav - total_reward_no_grav

print(f"\nè¯¯å·®æ”¹å–„:")
print(f"  æ— é‡åŠ›è¡¥å¿: {np.mean(errors_no_grav):.4f}å¼§åº¦ ({np.mean(errors_no_grav)*57.3:.2f}åº¦)")
print(f"  æœ‰é‡åŠ›è¡¥å¿: {np.mean(errors_with_grav):.4f}å¼§åº¦ ({np.mean(errors_with_grav)*57.3:.2f}åº¦)")
print(f"  è¯¯å·®é™ä½: {error_reduction:.4f}å¼§åº¦ ({error_reduction*57.3:.2f}åº¦)")
print(f"  æ”¹å–„ç‡: {error_reduction_pct:.2f}%")

print(f"\nå¥–åŠ±æ”¹å–„:")
print(f"  æ— é‡åŠ›è¡¥å¿: {total_reward_no_grav:.1f}")
print(f"  æœ‰é‡åŠ›è¡¥å¿: {total_reward_with_grav:.1f}")
print(f"  æ”¹å–„: {reward_improvement:+.1f}")

if error_reduction > 0:
    print(f"\nâœ… é‡åŠ›è¡¥å¿æœ‰æ•ˆï¼è¯¯å·®é™ä½ {error_reduction_pct:.2f}%")
else:
    print(f"\nâš ï¸  é‡åŠ›è¡¥å¿æ•ˆæœä¸æ˜æ˜¾")

# ================================================================================
# ç»˜åˆ¶å¯¹æ¯”å›¾
# ================================================================================
print("\n" + "=" * 70)
print("ğŸ“Š ç”Ÿæˆå¯¹æ¯”å›¾")
print("=" * 70)

times = np.arange(1000) * 0.001

fig, axes = plt.subplots(2, 1, figsize=(12, 8))

# è¯¯å·®å¯¹æ¯”
axes[0].plot(times, errors_no_grav, 'b-', 
             label=f'æ— é‡åŠ›è¡¥å¿ (å¹³å‡={np.mean(errors_no_grav):.4f})', 
             alpha=0.7, linewidth=1.5)
axes[0].plot(times, errors_with_grav, 'r-', 
             label=f'æœ‰é‡åŠ›è¡¥å¿ (å¹³å‡={np.mean(errors_with_grav):.4f})', 
             alpha=0.7, linewidth=1.5)
axes[0].set_xlabel('æ—¶é—´ (s)', fontsize=12)
axes[0].set_ylabel('è·Ÿè¸ªè¯¯å·® (å¼§åº¦)', fontsize=12)
axes[0].set_title(f'é‡åŠ›è¡¥å¿æ•ˆæœå¯¹æ¯” - è¯¯å·®é™ä½{error_reduction_pct:.1f}%', 
                  fontsize=14, fontweight='bold')
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# å¥–åŠ±å¯¹æ¯”
axes[1].plot(times, np.cumsum(rewards_no_grav), 'b-', 
             label=f'æ— é‡åŠ›è¡¥å¿ (æ€»è®¡={total_reward_no_grav:.1f})', 
             linewidth=2)
axes[1].plot(times, np.cumsum(rewards_with_grav), 'r-', 
             label=f'æœ‰é‡åŠ›è¡¥å¿ (æ€»è®¡={total_reward_with_grav:.1f})', 
             linewidth=2)
axes[1].set_xlabel('æ—¶é—´ (s)', fontsize=12)
axes[1].set_ylabel('ç´¯ç§¯å¥–åŠ±', fontsize=12)
axes[1].set_title('ç´¯ç§¯å¥–åŠ±å¯¹æ¯”', fontsize=14, fontweight='bold')
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('gravity_compensation_comparison.png', dpi=150, bbox_inches='tight')
print("âœ… å¯¹æ¯”å›¾å·²ä¿å­˜è‡³: gravity_compensation_comparison.png")

print("\n" + "=" * 70)
print("âœ… æµ‹è¯•å®Œæˆ")
print("=" * 70)

