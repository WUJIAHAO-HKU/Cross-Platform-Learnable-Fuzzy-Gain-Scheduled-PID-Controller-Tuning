#!/usr/bin/env python3
"""
è®­ç»ƒè‡ªé€‚åº”RL agent
ç›®æ ‡ï¼šå­¦ä¹ åœ¨çº¿è°ƒæ•´PIDå¢ç›Šä»¥åº”å¯¹æ‰°åŠ¨
"""

import os
import sys
from pathlib import Path
import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
from datetime import datetime

# å¯¼å…¥è‡ªé€‚åº”ç¯å¢ƒ
sys.path.append(str(Path(__file__).parent))
from adaptive_laikago_env import LaikagoAdaptiveEnv


def make_env(config, rank=0, gui=False):
    """åˆ›å»ºç¯å¢ƒ"""
    def _init():
        env = LaikagoAdaptiveEnv(config=config, gui=gui, use_meta_learning=True)
        env = Monitor(env)
        return env
    return _init


def train_adaptive_rl(
    total_timesteps=500000,
    n_envs=4,
    learning_rate=3e-4,
    batch_size=256,
    n_epochs=10,
    disturbance_type='random_force',
    save_dir='./logs/adaptive_rl',
    use_gpu=True
):
    """
    è®­ç»ƒè‡ªé€‚åº”RL agent
    
    Args:
        total_timesteps: æ€»è®­ç»ƒæ­¥æ•°
        n_envs: å¹¶è¡Œç¯å¢ƒæ•°é‡
        learning_rate: å­¦ä¹ ç‡
        batch_size: æ‰¹å¤§å°
        n_epochs: æ¯æ¬¡æ›´æ–°çš„è½®æ•°
        disturbance_type: æ‰°åŠ¨ç±»å‹
        save_dir: ä¿å­˜ç›®å½•
        use_gpu: æ˜¯å¦ä½¿ç”¨GPU
    """
    print("=" * 80)
    print("è‡ªé€‚åº”RLè®­ç»ƒå¼€å§‹")
    print("=" * 80)
    
    # é…ç½®
    config = {
        'max_steps': 5000,
        'init_kp': 0.5,  # å…ƒå­¦ä¹ ä¼šè¦†ç›–è¿™ä¸ªå€¼
        'init_kd': 0.1,
        'kp_range': (0.1, 2.0),
        'kd_range': (0.01, 0.5),
        'disturbance': {
            'type': disturbance_type,
            'force_range': (1.0, 3.0),  # è¾ƒå¼ºçš„æ‰°åŠ¨
            'force_interval': 800,
            'force_duration': 100
        }
    }
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_path = Path(save_dir) / f"adaptive_{disturbance_type}_{timestamp}"
    save_path.mkdir(parents=True, exist_ok=True)
    
    print(f"\nğŸ“ ä¿å­˜è·¯å¾„: {save_path}")
    print(f"ğŸ”§ é…ç½®:")
    print(f"   - å¹¶è¡Œç¯å¢ƒ: {n_envs}")
    print(f"   - æ‰°åŠ¨ç±»å‹: {disturbance_type}")
    print(f"   - æ‰°åŠ¨å¼ºåº¦: {config['disturbance']['force_range']}")
    print(f"   - å­¦ä¹ ç‡: {learning_rate}")
    print(f"   - GPU: {use_gpu}")
    
    # åˆ›å»ºç¯å¢ƒ
    print(f"\nğŸ—ï¸  åˆ›å»º{n_envs}ä¸ªå¹¶è¡Œç¯å¢ƒ...")
    if n_envs > 1:
        env = SubprocVecEnv([make_env(config, i) for i in range(n_envs)])
    else:
        env = DummyVecEnv([make_env(config)])
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
    print("ğŸ—ï¸  åˆ›å»ºè¯„ä¼°ç¯å¢ƒ...")
    eval_env = DummyVecEnv([make_env(config)])
    
    # è®¾ç½®è®¾å¤‡
    device = 'cuda' if use_gpu and torch.cuda.is_available() else 'cpu'
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºPPOæ¨¡å‹
    print("\nğŸ¤– åˆ›å»ºPPOæ¨¡å‹...")
    model = PPO(
        'MlpPolicy',
        env,
        learning_rate=learning_rate,
        n_steps=2048,
        batch_size=batch_size,
        n_epochs=n_epochs,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        vf_coef=0.5,
        max_grad_norm=0.5,
        verbose=1,
        device=device,
        tensorboard_log=str(save_path / 'tensorboard')
    )
    
    print(f"   - ç­–ç•¥ç½‘ç»œ: MlpPolicy")
    print(f"   - è§‚æµ‹ç»´åº¦: {env.observation_space.shape}")
    print(f"   - åŠ¨ä½œç»´åº¦: {env.action_space.shape}")
    
    # å›è°ƒå‡½æ•°
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=str(save_path / 'best_model'),
        log_path=str(save_path / 'eval_logs'),
        eval_freq=10000,
        n_eval_episodes=5,
        deterministic=True,
        render=False
    )
    
    checkpoint_callback = CheckpointCallback(
        save_freq=20000,
        save_path=str(save_path / 'checkpoints'),
        name_prefix='adaptive_rl'
    )
    
    # å¼€å§‹è®­ç»ƒ
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("=" * 80)
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=[eval_callback, checkpoint_callback],
            tb_log_name=f"adaptive_{disturbance_type}",
            progress_bar=False
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = save_path / 'final_model.zip'
        model.save(str(final_model_path))
        print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: {final_model_path}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        model.save(str(save_path / 'interrupted_model.zip'))
        print(f"ä¸­æ–­æ¨¡å‹å·²ä¿å­˜")
    
    finally:
        env.close()
        eval_env.close()
        print("âœ… ç¯å¢ƒå·²å…³é—­")
    
    return str(final_model_path)


def evaluate_adaptive_policy(model_path, n_episodes=10, disturbance_type='random_force', gui=False):
    """
    è¯„ä¼°è®­ç»ƒå¥½çš„è‡ªé€‚åº”ç­–ç•¥
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        n_episodes: è¯„ä¼°è½®æ•°
        disturbance_type: æ‰°åŠ¨ç±»å‹
        gui: æ˜¯å¦æ˜¾ç¤ºGUI
    """
    print("\n" + "=" * 80)
    print("è¯„ä¼°è‡ªé€‚åº”RLç­–ç•¥")
    print("=" * 80)
    
    # é…ç½®ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
    config = {
        'max_steps': 5000,
        'init_kp': 0.5,
        'init_kd': 0.1,
        'kp_range': (0.1, 2.0),
        'kd_range': (0.01, 0.5),
        'disturbance': {
            'type': disturbance_type,
            'force_range': (1.0, 3.0),
            'force_interval': 800,
            'force_duration': 100
        }
    }
    
    # åŠ è½½æ¨¡å‹
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    model = PPO.load(model_path)
    
    # åˆ›å»ºç¯å¢ƒ
    env = LaikagoAdaptiveEnv(config=config, gui=gui, use_meta_learning=True)
    
    # è¯„ä¼°
    total_rewards = []
    tracking_errors = []
    kp_adjustments = []
    kd_adjustments = []
    
    for episode in range(n_episodes):
        obs, _ = env.reset()
        episode_reward = 0
        episode_errors = []
        episode_kp = []
        episode_kd = []
        
        for step in range(config['max_steps']):
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            
            episode_reward += reward
            episode_errors.append(info['tracking_error'])
            episode_kp.append(info['current_kp'])
            episode_kd.append(info['current_kd'])
            
            if terminated or truncated:
                break
        
        total_rewards.append(episode_reward)
        tracking_errors.append(np.mean(episode_errors))
        kp_adjustments.append(episode_kp)
        kd_adjustments.append(episode_kd)
        
        print(f"Episode {episode+1}/{n_episodes}: "
              f"reward={episode_reward:.2f}, "
              f"avg_error={np.mean(episode_errors):.6f}, "
              f"final_Kp={episode_kp[-1]:.3f}, "
              f"final_Kd={episode_kd[-1]:.3f}")
    
    env.close()
    
    # ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("è¯„ä¼°ç»“æœç»Ÿè®¡")
    print("=" * 80)
    print(f"å¹³å‡å¥–åŠ±: {np.mean(total_rewards):.2f} Â± {np.std(total_rewards):.2f}")
    print(f"å¹³å‡è·Ÿè¸ªè¯¯å·®: {np.mean(tracking_errors):.6f} Â± {np.std(tracking_errors):.6f}")
    print(f"KpèŒƒå›´: [{np.min([np.min(k) for k in kp_adjustments]):.3f}, "
          f"{np.max([np.max(k) for k in kp_adjustments]):.3f}]")
    print(f"KdèŒƒå›´: [{np.min([np.min(k) for k in kd_adjustments]):.3f}, "
          f"{np.max([np.max(k) for k in kd_adjustments]):.3f}]")
    
    return {
        'mean_reward': np.mean(total_rewards),
        'mean_error': np.mean(tracking_errors),
        'kp_range': (np.min([np.min(k) for k in kp_adjustments]), 
                     np.max([np.max(k) for k in kp_adjustments])),
        'kd_range': (np.min([np.min(k) for k in kd_adjustments]), 
                     np.max([np.max(k) for k in kd_adjustments]))
    }


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='è®­ç»ƒè‡ªé€‚åº”RL agent')
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'eval'],
                       help='è®­ç»ƒæˆ–è¯„ä¼°æ¨¡å¼')
    parser.add_argument('--timesteps', type=int, default=500000,
                       help='æ€»è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--n_envs', type=int, default=4,
                       help='å¹¶è¡Œç¯å¢ƒæ•°é‡')
    parser.add_argument('--disturbance', type=str, default='random_force',
                       choices=['random_force', 'payload', 'terrain'],
                       help='æ‰°åŠ¨ç±»å‹')
    parser.add_argument('--model', type=str, default=None,
                       help='è¯„ä¼°æ¨¡å¼ä¸‹çš„æ¨¡å‹è·¯å¾„')
    parser.add_argument('--gui', action='store_true',
                       help='è¯„ä¼°æ—¶æ˜¾ç¤ºGUI')
    parser.add_argument('--gpu', action='store_true',
                       help='ä½¿ç”¨GPUè®­ç»ƒ')
    
    args = parser.parse_args()
    
    if args.mode == 'train':
        model_path = train_adaptive_rl(
            total_timesteps=args.timesteps,
            n_envs=args.n_envs,
            disturbance_type=args.disturbance,
            use_gpu=args.gpu
        )
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æ¨¡å‹è·¯å¾„: {model_path}")
        
    elif args.mode == 'eval':
        if args.model is None:
            print("âŒ è¯„ä¼°æ¨¡å¼éœ€è¦æä¾›--modelå‚æ•°")
            sys.exit(1)
        
        evaluate_adaptive_policy(
            model_path=args.model,
            n_episodes=10,
            disturbance_type=args.disturbance,
            gui=args.gui
        )

