#!/usr/bin/env python3
"""
å¯¹æ¯”å®éªŒï¼šå›ºå®šPID vs è‡ªé€‚åº”RL
è¯„ä¼°åœ¨ä¸åŒæ‰°åŠ¨ä¸‹çš„æ€§èƒ½å·®å¼‚
"""

import numpy as np
import matplotlib.pyplot as plt
import sys
from pathlib import Path
from stable_baselines3 import PPO

sys.path.append(str(Path(__file__).parent))
from adaptive_laikago_env import LaikagoAdaptiveEnv


def evaluate_fixed_pid(config, n_episodes=10, gui=False):
    """
    è¯„ä¼°å›ºå®šPIDï¼ˆæ— RLï¼Œdelta_action=0ï¼‰
    
    Args:
        config: ç¯å¢ƒé…ç½®
        n_episodes: è¯„ä¼°è½®æ•°
        gui: æ˜¯å¦æ˜¾ç¤ºGUI
    
    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print("\n" + "=" * 80)
    print("è¯„ä¼°å›ºå®šPIDï¼ˆæ— RLè°ƒæ•´ï¼‰")
    print("=" * 80)
    
    env = LaikagoAdaptiveEnv(config=config, gui=gui, use_meta_learning=True)
    
    all_rewards = []
    all_tracking_errors = []
    all_orientation_errors = []
    
    for episode in range(n_episodes):
        obs, _ = env.reset()
        episode_reward = 0
        episode_errors = []
        episode_orientation = []
        
        for step in range(config['max_steps']):
            # å›ºå®šPIDï¼šä¸è°ƒæ•´å¢ç›Šï¼ˆaction=0ï¼‰
            action = np.zeros(2)
            obs, reward, terminated, truncated, info = env.step(action)
            
            episode_reward += reward
            episode_errors.append(info['tracking_error'])
            episode_orientation.append(info['orientation_penalty'])
            
            if terminated or truncated:
                break
        
        all_rewards.append(episode_reward)
        all_tracking_errors.append(np.mean(episode_errors))
        all_orientation_errors.append(np.mean(episode_orientation))
        
        print(f"  Episode {episode+1}/{n_episodes}: "
              f"reward={episode_reward:.2f}, "
              f"tracking_error={np.mean(episode_errors):.6f}, "
              f"orientation_error={np.mean(episode_orientation):.6f}")
    
    env.close()
    
    results = {
        'mean_reward': np.mean(all_rewards),
        'std_reward': np.std(all_rewards),
        'mean_tracking_error': np.mean(all_tracking_errors),
        'std_tracking_error': np.std(all_tracking_errors),
        'mean_orientation_error': np.mean(all_orientation_errors),
        'std_orientation_error': np.std(all_orientation_errors)
    }
    
    print(f"\nå›ºå®šPIDç»“æœ:")
    print(f"  å¹³å‡å¥–åŠ±: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
    print(f"  è·Ÿè¸ªè¯¯å·®: {results['mean_tracking_error']:.6f} Â± {results['std_tracking_error']:.6f}")
    print(f"  å§¿æ€è¯¯å·®: {results['mean_orientation_error']:.6f} Â± {results['std_orientation_error']:.6f}")
    
    return results


def evaluate_adaptive_rl(model_path, config, n_episodes=10, gui=False):
    """
    è¯„ä¼°è‡ªé€‚åº”RL
    
    Args:
        model_path: è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„
        config: ç¯å¢ƒé…ç½®
        n_episodes: è¯„ä¼°è½®æ•°
        gui: æ˜¯å¦æ˜¾ç¤ºGUI
    
    Returns:
        results: è¯„ä¼°ç»“æœå­—å…¸
    """
    print("\n" + "=" * 80)
    print("è¯„ä¼°è‡ªé€‚åº”RL")
    print("=" * 80)
    
    model = PPO.load(model_path)
    env = LaikagoAdaptiveEnv(config=config, gui=gui, use_meta_learning=True)
    
    all_rewards = []
    all_tracking_errors = []
    all_orientation_errors = []
    all_kp_adjustments = []
    all_kd_adjustments = []
    
    for episode in range(n_episodes):
        obs, _ = env.reset()
        episode_reward = 0
        episode_errors = []
        episode_orientation = []
        episode_kp = []
        episode_kd = []
        
        for step in range(config['max_steps']):
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            
            episode_reward += reward
            episode_errors.append(info['tracking_error'])
            episode_orientation.append(info['orientation_penalty'])
            episode_kp.append(info['current_kp'])
            episode_kd.append(info['current_kd'])
            
            if terminated or truncated:
                break
        
        all_rewards.append(episode_reward)
        all_tracking_errors.append(np.mean(episode_errors))
        all_orientation_errors.append(np.mean(episode_orientation))
        all_kp_adjustments.append(episode_kp)
        all_kd_adjustments.append(episode_kd)
        
        print(f"  Episode {episode+1}/{n_episodes}: "
              f"reward={episode_reward:.2f}, "
              f"tracking_error={np.mean(episode_errors):.6f}, "
              f"final_Kp={episode_kp[-1]:.3f}, "
              f"final_Kd={episode_kd[-1]:.3f}")
    
    env.close()
    
    results = {
        'mean_reward': np.mean(all_rewards),
        'std_reward': np.std(all_rewards),
        'mean_tracking_error': np.mean(all_tracking_errors),
        'std_tracking_error': np.std(all_tracking_errors),
        'mean_orientation_error': np.mean(all_orientation_errors),
        'std_orientation_error': np.std(all_orientation_errors),
        'kp_adjustments': all_kp_adjustments,
        'kd_adjustments': all_kd_adjustments,
        'kp_range': (np.min([np.min(k) for k in all_kp_adjustments]),
                     np.max([np.max(k) for k in all_kp_adjustments])),
        'kd_range': (np.min([np.min(k) for k in all_kd_adjustments]),
                     np.max([np.max(k) for k in all_kd_adjustments]))
    }
    
    print(f"\nè‡ªé€‚åº”RLç»“æœ:")
    print(f"  å¹³å‡å¥–åŠ±: {results['mean_reward']:.2f} Â± {results['std_reward']:.2f}")
    print(f"  è·Ÿè¸ªè¯¯å·®: {results['mean_tracking_error']:.6f} Â± {results['std_tracking_error']:.6f}")
    print(f"  å§¿æ€è¯¯å·®: {results['mean_orientation_error']:.6f} Â± {results['std_orientation_error']:.6f}")
    print(f"  KpèŒƒå›´: [{results['kp_range'][0]:.3f}, {results['kp_range'][1]:.3f}]")
    print(f"  KdèŒƒå›´: [{results['kd_range'][0]:.3f}, {results['kd_range'][1]:.3f}]")
    
    return results


def compare_methods(scenario_name, model_path=None, n_episodes=10, gui=False):
    """
    å¯¹æ¯”å›ºå®šPID vs è‡ªé€‚åº”RL
    
    Args:
        scenario_name: æ‰°åŠ¨åœºæ™¯åç§°
        model_path: è‡ªé€‚åº”RLæ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœNoneåˆ™åªè¯„ä¼°å›ºå®šPIDï¼‰
        n_episodes: è¯„ä¼°è½®æ•°
        gui: æ˜¯å¦æ˜¾ç¤ºGUI
    """
    from train_multi_disturbance import DISTURBANCE_SCENARIOS
    
    print("=" * 80)
    print(f"å¯¹æ¯”å®éªŒï¼š{scenario_name}")
    print(f"æè¿°ï¼š{DISTURBANCE_SCENARIOS[scenario_name]['description']}")
    print("=" * 80)
    
    # é…ç½®
    config = {
        'max_steps': 5000,
        'init_kp': 0.5,
        'init_kd': 0.1,
        'kp_range': (0.1, 2.0),
        'kd_range': (0.01, 0.5),
        'disturbance': DISTURBANCE_SCENARIOS[scenario_name]
    }
    
    # è¯„ä¼°å›ºå®šPID
    fixed_results = evaluate_fixed_pid(config, n_episodes, gui)
    
    # è¯„ä¼°è‡ªé€‚åº”RLï¼ˆå¦‚æœæä¾›äº†æ¨¡å‹ï¼‰
    if model_path:
        adaptive_results = evaluate_adaptive_rl(model_path, config, n_episodes, gui)
        
        # è®¡ç®—æ”¹å–„ç‡
        reward_improvement = ((adaptive_results['mean_reward'] - fixed_results['mean_reward']) 
                             / abs(fixed_results['mean_reward'])) * 100
        error_improvement = ((fixed_results['mean_tracking_error'] - adaptive_results['mean_tracking_error']) 
                            / fixed_results['mean_tracking_error']) * 100
        
        print("\n" + "=" * 80)
        print("æ€§èƒ½å¯¹æ¯”")
        print("=" * 80)
        print(f"å¥–åŠ±æ”¹å–„: {reward_improvement:+.2f}%")
        print(f"è·Ÿè¸ªè¯¯å·®æ”¹å–„: {error_improvement:+.2f}%")
        
        return fixed_results, adaptive_results, reward_improvement, error_improvement
    
    else:
        return fixed_results, None, None, None


def plot_comparison(scenario_name, fixed_results, adaptive_results):
    """ç”Ÿæˆå¯¹æ¯”å›¾è¡¨"""
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # å›¾1: å¥–åŠ±å¯¹æ¯”
    methods = ['Fixed PID', 'Adaptive RL']
    rewards = [fixed_results['mean_reward'], adaptive_results['mean_reward']]
    reward_stds = [fixed_results['std_reward'], adaptive_results['std_reward']]
    
    axes[0].bar(methods, rewards, yerr=reward_stds, capsize=5, alpha=0.7)
    axes[0].set_ylabel('Mean Reward')
    axes[0].set_title(f'{scenario_name}: Reward Comparison')
    axes[0].grid(axis='y', alpha=0.3)
    
    # å›¾2: è·Ÿè¸ªè¯¯å·®å¯¹æ¯”
    errors = [fixed_results['mean_tracking_error'], adaptive_results['mean_tracking_error']]
    error_stds = [fixed_results['std_tracking_error'], adaptive_results['std_tracking_error']]
    
    axes[1].bar(methods, errors, yerr=error_stds, capsize=5, alpha=0.7, color=['orange', 'green'])
    axes[1].set_ylabel('Tracking Error (rad)')
    axes[1].set_title(f'{scenario_name}: Tracking Error Comparison')
    axes[1].grid(axis='y', alpha=0.3)
    
    # å›¾3: å¢ç›Šè°ƒæ•´ç¤ºä¾‹ï¼ˆç¬¬ä¸€ä¸ªepisodeï¼‰
    if adaptive_results and 'kp_adjustments' in adaptive_results:
        kp = adaptive_results['kp_adjustments'][0]
        kd = adaptive_results['kd_adjustments'][0]
        
        axes[2].plot(kp, label='Kp', alpha=0.7)
        axes[2].plot(kd, label='Kd', alpha=0.7)
        axes[2].axhline(0.5, color='r', linestyle='--', alpha=0.5, label='Initial Kp')
        axes[2].axhline(0.1, color='b', linestyle='--', alpha=0.5, label='Initial Kd')
        axes[2].set_xlabel('Step')
        axes[2].set_ylabel('Gain Value')
        axes[2].set_title(f'{scenario_name}: Gain Adjustments')
        axes[2].legend()
        axes[2].grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'comparison_{scenario_name}.png', dpi=150)
    print(f"\nğŸ“Š å›¾è¡¨å·²ä¿å­˜: comparison_{scenario_name}.png")
    plt.close()


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='å¯¹æ¯”å›ºå®šPID vs è‡ªé€‚åº”RL')
    parser.add_argument('--scenario', type=str, default='random_force',
                       choices=['random_force', 'payload', 'terrain', 'param_uncertainty', 'mixed'],
                       help='æ‰°åŠ¨åœºæ™¯')
    parser.add_argument('--model', type=str, default=None,
                       help='è‡ªé€‚åº”RLæ¨¡å‹è·¯å¾„')
    parser.add_argument('--n_episodes', type=int, default=10,
                       help='è¯„ä¼°è½®æ•°')
    parser.add_argument('--gui', action='store_true',
                       help='æ˜¾ç¤ºGUI')
    parser.add_argument('--plot', action='store_true',
                       help='ç”Ÿæˆå¯¹æ¯”å›¾è¡¨')
    
    args = parser.parse_args()
    
    # è¿è¡Œå¯¹æ¯”å®éªŒ
    fixed_results, adaptive_results, reward_imp, error_imp = compare_methods(
        scenario_name=args.scenario,
        model_path=args.model,
        n_episodes=args.n_episodes,
        gui=args.gui
    )
    
    # ç”Ÿæˆå›¾è¡¨
    if args.plot and adaptive_results:
        plot_comparison(args.scenario, fixed_results, adaptive_results)

