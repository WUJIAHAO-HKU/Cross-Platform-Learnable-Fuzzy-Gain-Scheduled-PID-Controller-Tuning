"""
å¯è§†åŒ–æœºæ¢°è‡‚è¿åŠ¨ - å¯¹æ¯”çº¯PIDå’ŒRL+PID
"""

import yaml
import numpy as np
import argparse
import pybullet as p
from stable_baselines3 import PPO
from envs.franka_env import FrankaRLPIDEnv
import time

parser = argparse.ArgumentParser(description='å¯è§†åŒ–æœºæ¢°è‡‚è¿åŠ¨')
parser.add_argument('--mode', choices=['pid', 'rl', 'compare'], default='compare',
                    help='å¯è§†åŒ–æ¨¡å¼: pid(çº¯PID), rl(RL+PID), compare(å¯¹æ¯”)')
parser.add_argument('--steps', type=int, default=3000, help='è¿è¡Œæ­¥æ•°')
parser.add_argument('--speed', type=float, default=1.0, help='æ’­æ”¾é€Ÿåº¦å€ç‡ï¼ˆ1.0=æ­£å¸¸ï¼Œ0.5=æ…¢é€Ÿï¼Œ2.0=å¿«é€Ÿï¼‰')
args = parser.parse_args()

# åŠ è½½é…ç½®
with open('configs/stage1_final.yaml', 'r') as f:
    config = yaml.safe_load(f)

print("\n" + "=" * 70)
print("ğŸ¬ Franka Panda æœºæ¢°è‡‚è¿åŠ¨å¯è§†åŒ–")
print("=" * 70)
print(f"\næ¨¡å¼: {args.mode}")
print(f"æ­¥æ•°: {args.steps}")
print(f"é€Ÿåº¦: {args.speed}x")
print("\næ“ä½œæç¤º:")
print("  - é¼ æ ‡å·¦é”®æ‹–åŠ¨ï¼šæ—‹è½¬è§†è§’")
print("  - é¼ æ ‡å³é”®æ‹–åŠ¨ï¼šå¹³ç§»è§†è§’")
print("  - é¼ æ ‡æ»šè½®ï¼šç¼©æ”¾")
print("  - Ctrl+Cï¼šåœæ­¢è¿è¡Œ")
print("=" * 70 + "\n")

if args.mode == 'compare':
    print("ğŸ“Š å¯¹æ¯”æ¨¡å¼ï¼šå°†å…ˆè¿è¡Œçº¯PIDï¼Œç„¶åè¿è¡ŒRL+PID")
    print("    è§‚å¯Ÿä¸¤è€…çš„è·Ÿè¸ªæ€§èƒ½å·®å¼‚\n")

def run_controller(env, model=None, name="æ§åˆ¶å™¨", steps=3000, speed=1.0):
    """è¿è¡Œæ§åˆ¶å™¨å¹¶æ˜¾ç¤ºç»Ÿè®¡"""
    print(f"\nğŸ¬ æ­£åœ¨è¿è¡Œ: {name}")
    print("-" * 70)
    
    obs, _ = env.reset()
    
    # é…ç½®ç›¸æœºè§†è§’
    p.resetDebugVisualizerCamera(
        cameraDistance=1.5,
        cameraYaw=45,
        cameraPitch=-30,
        cameraTargetPosition=[0, 0, 0.4]
    )
    
    errors = []
    rewards = []
    actions_norm = []
    
    sleep_time = 0.001 / speed  # æ ¹æ®é€Ÿåº¦å€ç‡è°ƒæ•´sleepæ—¶é—´
    
    for step in range(steps):
        if model is None:
            # çº¯PID
            action = np.zeros(7, dtype=np.float32)
        else:
            # RL+PID
            action, _ = model.predict(obs, deterministic=True)
        
        obs, reward, terminated, truncated, info = env.step(action)
        
        errors.append(info['err_norm'])
        rewards.append(reward)
        if model is not None:
            actions_norm.append(np.linalg.norm(action))
        
        # æ§åˆ¶æ’­æ”¾é€Ÿåº¦
        time.sleep(sleep_time)
        
        # æ¯500æ­¥æ˜¾ç¤ºä¸€æ¬¡çŠ¶æ€
        if (step + 1) % 500 == 0:
            avg_err = np.mean(errors[max(0, step-499):step+1])
            print(f"  Step {step+1:4d}/{steps}: å¹³å‡è¯¯å·®={avg_err:.4f}å¼§åº¦, å³æ—¶å¥–åŠ±={reward:6.2f}")
    
    # æ˜¾ç¤ºç»Ÿè®¡
    print("\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
    print(f"  å¹³å‡è¯¯å·®: {np.mean(errors):.4f}å¼§åº¦ ({np.mean(errors)*57.3:.2f}åº¦)")
    print(f"  ä¸­ä½è¯¯å·®: {np.median(errors):.4f}å¼§åº¦")
    print(f"  æœ€å¤§è¯¯å·®: {np.max(errors):.4f}å¼§åº¦")
    print(f"  æœ€å°è¯¯å·®: {np.min(errors):.4f}å¼§åº¦")
    print(f"  æ€»å¥–åŠ±: {sum(rewards):.1f}")
    
    if model is not None and len(actions_norm) > 0:
        print(f"\n  RLè¡¥å¿:")
        print(f"    å¹³å‡actionèŒƒæ•°: {np.mean(actions_norm):.4f}")
        print(f"    æœ€å¤§actionèŒƒæ•°: {np.max(actions_norm):.4f}")
    
    return errors, rewards

# åˆ›å»ºç¯å¢ƒï¼ˆGUIæ¨¡å¼ï¼‰
env = FrankaRLPIDEnv(config, gui=True)

try:
    if args.mode == 'pid':
        # åªè¿è¡Œçº¯PID
        run_controller(env, model=None, name="çº¯PIDæ§åˆ¶", 
                      steps=args.steps, speed=args.speed)
    
    elif args.mode == 'rl':
        # åªè¿è¡ŒRL+PID
        model = PPO.load("./logs/rl_pid_ppo_final")
        print("âœ… RLæ¨¡å‹åŠ è½½æˆåŠŸ")
        run_controller(env, model=model, name="RL+PIDæ§åˆ¶", 
                      steps=args.steps, speed=args.speed)
    
    elif args.mode == 'compare':
        # å¯¹æ¯”æ¨¡å¼
        print("\n" + "=" * 70)
        print("ç¬¬1éƒ¨åˆ†ï¼šçº¯PIDæ§åˆ¶")
        print("=" * 70)
        errors_pid, rewards_pid = run_controller(
            env, model=None, name="çº¯PIDæ§åˆ¶", 
            steps=args.steps, speed=args.speed
        )
        
        print("\n" + "=" * 70)
        print("ç¬¬2éƒ¨åˆ†ï¼šRL+PIDæ§åˆ¶")
        print("=" * 70)
        model = PPO.load("./logs/rl_pid_ppo_final")
        print("âœ… RLæ¨¡å‹åŠ è½½æˆåŠŸ")
        errors_rl, rewards_rl = run_controller(
            env, model=model, name="RL+PIDæ§åˆ¶", 
            steps=args.steps, speed=args.speed
        )
        
        # å¯¹æ¯”ç»“æœ
        print("\n" + "=" * 70)
        print("ğŸ“Š å¯¹æ¯”ç»“æœ")
        print("=" * 70)
        
        error_reduction = np.mean(errors_pid) - np.mean(errors_rl)
        reward_improvement = sum(rewards_rl) - sum(rewards_pid)
        
        print(f"\n  çº¯PIDå¹³å‡è¯¯å·®:   {np.mean(errors_pid):.4f}å¼§åº¦")
        print(f"  RL+PIDå¹³å‡è¯¯å·®:  {np.mean(errors_rl):.4f}å¼§åº¦")
        print(f"  è¯¯å·®é™ä½:        {error_reduction:.4f}å¼§åº¦ ({error_reduction*57.3:.2f}åº¦)")
        print(f"  è¯¯å·®æ”¹å–„ç‡:      {(error_reduction / np.mean(errors_pid) * 100):.2f}%")
        
        print(f"\n  çº¯PIDæ€»å¥–åŠ±:     {sum(rewards_pid):.1f}")
        print(f"  RL+PIDæ€»å¥–åŠ±:    {sum(rewards_rl):.1f}")
        print(f"  å¥–åŠ±æ”¹å–„:        {reward_improvement:+.1f} ({(reward_improvement / abs(sum(rewards_pid)) * 100):+.2f}%)")
        
        if error_reduction > 0:
            print(f"\n  âœ… RL+PIDç›¸æ¯”çº¯PIDæ€§èƒ½æå‡ {(error_reduction / np.mean(errors_pid) * 100):.2f}%")
        else:
            print(f"\n  âš ï¸  æ³¨æ„ï¼šRL+PIDæœªæ˜¾ç¤ºæ€§èƒ½æå‡")

except KeyboardInterrupt:
    print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")

finally:
    env.close()
    print("\nâœ… å¯è§†åŒ–å®Œæˆ")
    print("=" * 70 + "\n")

