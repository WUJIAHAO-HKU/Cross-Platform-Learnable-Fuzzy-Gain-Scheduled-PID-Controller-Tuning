# RLæ€§èƒ½æå‡ä¸æ˜æ˜¾çš„åŸå› åˆ†æä¸ä¼˜åŒ–æ–¹æ¡ˆ

## ğŸ” å½“å‰é—®é¢˜

### æ€§èƒ½è¡¨ç°ä¸ä½³
- **Franka Panda**: Overall +10.9% (è¿˜å¯ä»¥ï¼Œä½†é€å…³èŠ‚æå‡æœ‰é™ï¼Œéƒ¨åˆ†å…³èŠ‚é€€åŒ–)
- **Laikago**: Overall +0.1% (**å‡ ä¹æ²¡æœ‰æ”¹å–„ï¼**)

###å…³é”®è§‚å¯Ÿ
1. âœ… Franka J2æœ‰72.6%æ”¹å–„ - **è¯´æ˜RLæ˜¯æœ‰æ•ˆçš„**
2. âŒ Laikagoæ•´ä½“å‡ ä¹æ— æ”¹å–„ - **è¯´æ˜æ³›åŒ–èƒ½åŠ›å·®**
3. âŒ Frankaéƒ¨åˆ†å…³èŠ‚é€€åŒ– (J1, J3-J6) - **è¯´æ˜RLåœ¨æŸäº›æƒ…å†µä¸‹åè€Œæœ‰å®³**
4. âš ï¸ æ”¹å–„é«˜åº¦ä¸å‡åŒ€ - **è¯´æ˜å¥–åŠ±å‡½æ•°æˆ–è®­ç»ƒç­–ç•¥æœ‰é—®é¢˜**

---

## ğŸ¯ æ ¹æœ¬åŸå› åˆ†æ

### åŸå› 1: RLè®­ç»ƒé…ç½®ä¸¥é‡ä¸è¶³ âŒâŒâŒ (æœ€å…³é”®)

#### å½“å‰é…ç½®ï¼ˆæ¥è‡ªä»£ç æ£€æŸ¥ï¼‰:
```python
total_timesteps = 200,000    # ä»…20ä¸‡æ­¥
n_envs = 4                   # 4ä¸ªå¹¶è¡Œç¯å¢ƒ
n_steps = 512                # æ¯ä¸ªç¯å¢ƒ512æ­¥
batch_size = 64              # Batch sizeä»…64
learning_rate = 3e-4         # å›ºå®šå­¦ä¹ ç‡
```

#### é—®é¢˜è¯Šæ–­:

| å‚æ•° | å½“å‰å€¼ | é—®é¢˜ | å»ºè®®å€¼ | ç†ç”± |
|------|--------|------|--------|------|
| **total_timesteps** | 200k | âŒ **ä¸¥é‡ä¸è¶³** | **1M-2M** | PPOé€šå¸¸éœ€è¦ç™¾ä¸‡çº§æ­¥æ•°æ‰èƒ½å……åˆ†å­¦ä¹  |
| **n_steps** | 512 | âŒ **å¤ªå°** | **2048** | PPOè®ºæ–‡æ¨è2048ï¼Œæ›´å¥½çš„è½¨è¿¹ç‰‡æ®µ |
| **batch_size** | 64 | âŒ **å¤ªå°** | **2048** | ä¸n_stepsç›¸ç­‰ï¼Œä¿è¯å®Œæ•´trajectory |
| **learning_rate** | 3e-4 | âš ï¸ **å¯èƒ½å¤ªå¤§** | **1e-4æˆ–çº¿æ€§è¡°å‡** | æ›´ç¨³å®šçš„å­¦ä¹  |
| **n_envs** | 4 | âš ï¸ **å¯ä»¥æ›´å¤š** | **8-16** | æ›´å¥½çš„é‡‡æ ·å¤šæ ·æ€§ |
| **è®­ç»ƒæ—¶é•¿** | 2.5å°æ—¶ | âŒ **å¤ªçŸ­** | **10-20å°æ—¶** | å……åˆ†å­¦ä¹ éœ€è¦æ›´é•¿æ—¶é—´ |

**ç»“è®º**: æ‚¨çš„è®­ç»ƒæ—¶é—´ä»…ä¸ºæ ‡å‡†PPOè®­ç»ƒçš„**1/5åˆ°1/10**ï¼è¿™æ˜¯æ€§èƒ½å·®çš„ä¸»è¦åŸå› ã€‚

### åŸå› 2: å¥–åŠ±å‡½æ•°è®¾è®¡é—®é¢˜ âŒ

#### å½“å‰å¥–åŠ±å‡½æ•°:
```python
reward = (
    -10.0 * tracking_error_norm  # ä¸»è¦ï¼šè·Ÿè¸ªè¯¯å·®
    -0.1 * velocity_norm         # æ¬¡è¦ï¼šé€Ÿåº¦å¹³æ»‘
    -0.1 * action_norm           # æ¬¡è¦ï¼šåŠ¨ä½œå¹³æ»‘
)
reward = np.clip(reward, -100.0, 10.0)
```

#### é—®é¢˜:
1. **æƒé‡å›ºå®š** - ä¸åŒæœºå™¨äººï¼ˆFranka vs Laikagoï¼‰åº”è¯¥ç”¨ä¸åŒæƒé‡
2. **æ²¡æœ‰é¼“åŠ±æ”¹å–„** - åªæƒ©ç½šè¯¯å·®ï¼Œæ²¡æœ‰å¥–åŠ±ç›¸å¯¹æ”¹å–„
3. **è£å‰ªèŒƒå›´ä¸åˆç†** - [-100, 10]ä¸å¯¹ç§°ï¼Œå¯èƒ½å¯¼è‡´å­¦ä¹ åå·®
4. **æ²¡æœ‰é€å…³èŠ‚å¥–åŠ±** - æ‰€æœ‰å…³èŠ‚è¯¯å·®è¢«å¹³å‡ï¼Œæ— æ³•é’ˆå¯¹æ€§ä¼˜åŒ–

### åŸå› 3: Meta-PIDå·²ç»å¾ˆå¥½ï¼ˆè¿™æ˜¯å¥½äº‹ï¼Œä½†é™åˆ¶æ”¹è¿›ç©ºé—´ï¼‰âœ…

- æ‚¨çš„Meta-PIDè®­ç»ƒéå¸¸æˆåŠŸï¼ˆ96.3%æ”¹è¿›ï¼‰
- Base errorå·²ç»å¾ˆä½
- RLåªèƒ½åœ¨å·²ç»å¾ˆå¥½çš„åŸºç¡€ä¸Šåšå¾®è°ƒ
- **è¿™ä¸æ˜¯é—®é¢˜ï¼Œè€Œæ˜¯è¯´æ˜meta-learningå¾ˆæˆåŠŸï¼**

### åŸå› 4: ç¼ºä¹é’ˆå¯¹æ€§çš„è®­ç»ƒç­–ç•¥ âŒ

1. **æ²¡æœ‰åŒºåˆ†ä¸åŒæœºå™¨äºº** - Frankaå’ŒLaikagoåº”è¯¥ç”¨ä¸åŒçš„è®­ç»ƒç­–ç•¥
2. **æ²¡æœ‰é’ˆå¯¹å›°éš¾åœºæ™¯è®­ç»ƒ** - åº”è¯¥å¢åŠ å¹²æ‰°ã€è´Ÿè½½ç­‰æŒ‘æˆ˜
3. **æ²¡æœ‰è¯¾ç¨‹å­¦ä¹ ** - åº”è¯¥ä»ç®€å•åˆ°å›°éš¾é€æ­¥è®­ç»ƒ

---

## ğŸ’¡ ä¼˜åŒ–æ–¹æ¡ˆ

### ğŸš€ æ–¹æ¡ˆA: åŸºç¡€ä¼˜åŒ–ï¼ˆå¿«é€Ÿè§æ•ˆï¼Œå»ºè®®ä¼˜å…ˆï¼‰

#### 1. å¢åŠ è®­ç»ƒæ­¥æ•° â­â­â­ (æœ€é‡è¦)
```python
# train_meta_rl_combined.py
total_timesteps = 1_000_000  # ä»200kå¢åŠ åˆ°1M (5å€)
# æˆ–è€…æ›´æ¿€è¿›ï¼š
total_timesteps = 2_000_000  # 2Mæ­¥ (10å€)
```

**é¢„æœŸæ•ˆæœ**: Frankaæ”¹å–„ 10.9% â†’ **20-30%**, Laikagoæ”¹å–„ 0.1% â†’ **5-10%**

#### 2. ä¼˜åŒ–PPOè¶…å‚æ•°
```python
model = PPO(
    'MlpPolicy', env,
    learning_rate=1e-4,        # ä»3e-4é™ä½åˆ°1e-4
    n_steps=2048,              # ä»512å¢åŠ åˆ°2048
    batch_size=2048,           # ä»64å¢åŠ åˆ°2048
    n_epochs=10,               # ä¿æŒ10
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.01,             # å¯ä»¥å°è¯•0.02å¢åŠ æ¢ç´¢
    vf_coef=0.5,
    max_grad_norm=0.5,
    verbose=1,
    tensorboard_log="./logs/tensorboard/"
)
```

#### 3. å¢åŠ å¹¶è¡Œç¯å¢ƒ
```python
n_envs = 8  # ä»4å¢åŠ åˆ°8
# æˆ–æ›´å¤šï¼š
n_envs = 16  # å¦‚æœGPUå†…å­˜å…è®¸
```

**å®æ–½éš¾åº¦**: â­ (ç®€å•ï¼Œåªéœ€ä¿®æ”¹å‡ è¡Œä»£ç )  
**é¢„æœŸæ—¶é•¿**: 10-20å°æ—¶è®­ç»ƒ  
**é¢„æœŸæ”¹å–„**: Franka +20-30%, Laikago +5-10%

### ğŸ¯ æ–¹æ¡ˆB: æ”¹è¿›å¥–åŠ±å‡½æ•°ï¼ˆä¸­ç­‰æ•ˆæœï¼‰

#### æ–°å¥–åŠ±å‡½æ•°è®¾è®¡:
```python
def _compute_reward_improved(self, error, qd, action, prev_error):
    """æ”¹è¿›çš„å¥–åŠ±å‡½æ•°"""
    # 1. å½“å‰è¯¯å·®ï¼ˆå½’ä¸€åŒ–ï¼‰
    error_norm = np.linalg.norm(error) / np.sqrt(self.n_dof)
    
    # 2. è¯¯å·®æ”¹å–„ï¼ˆç›¸å¯¹äºä¸Šä¸€æ­¥ï¼‰
    if prev_error is not None:
        prev_error_norm = np.linalg.norm(prev_error) / np.sqrt(self.n_dof)
        improvement = prev_error_norm - error_norm  # æ­£å€¼=æ”¹å–„
    else:
        improvement = 0
    
    # 3. é€å…³èŠ‚å¥–åŠ±ï¼ˆå¯¹é«˜è¯¯å·®å…³èŠ‚ç»™äºˆæ›´å¤šå…³æ³¨ï¼‰
    per_joint_error = np.abs(error)
    high_error_joints = per_joint_error > np.mean(per_joint_error)
    joint_penalty = -5.0 * np.mean(per_joint_error[high_error_joints])
    
    # 4. åŠ¨ä½œå¹³æ»‘
    action_norm = np.linalg.norm(action)
    velocity_norm = np.linalg.norm(qd) / np.sqrt(self.n_dof)
    
    # 5. ç»„åˆå¥–åŠ±ï¼ˆæƒé‡å¯è°ƒï¼‰
    reward = (
        -10.0 * error_norm       # å½“å‰è¯¯å·®æƒ©ç½š
        + 20.0 * improvement     # æ”¹å–„å¥–åŠ±ï¼ˆæƒé‡å¤§ï¼Œé¼“åŠ±æ”¹å–„ï¼‰
        + joint_penalty          # é«˜è¯¯å·®å…³èŠ‚é¢å¤–æƒ©ç½š
        - 0.1 * action_norm      # åŠ¨ä½œå¹³æ»‘
        - 0.1 * velocity_norm    # é€Ÿåº¦å¹³æ»‘
    )
    
    # 6. é‡Œç¨‹ç¢‘å¥–åŠ±
    if error_norm < 0.1:
        reward += 10.0  # è¾¾åˆ°ä½è¯¯å·®æ—¶çš„å¥–åŠ±
    
    # 7. å¯¹ç§°è£å‰ª
    reward = np.clip(reward, -50.0, 50.0)
    
    return reward
```

**å…³é”®æ”¹è¿›**:
1. âœ… å¥–åŠ±ç›¸å¯¹æ”¹å–„ï¼ˆé¼“åŠ±å­¦ä¹ ï¼‰
2. âœ… é’ˆå¯¹é«˜è¯¯å·®å…³èŠ‚ï¼ˆé’ˆå¯¹æ€§ä¼˜åŒ–ï¼‰
3. âœ… é‡Œç¨‹ç¢‘å¥–åŠ±ï¼ˆé¼“åŠ±è¾¾æˆç›®æ ‡ï¼‰
4. âœ… å¯¹ç§°è£å‰ªï¼ˆæ›´å¹³è¡¡çš„å­¦ä¹ ï¼‰

**é¢„æœŸæ”¹å–„**: é¢å¤–+5-10%

### ğŸ† æ–¹æ¡ˆC: é«˜çº§ä¼˜åŒ–ï¼ˆæ•ˆæœæœ€å¥½ï¼Œä½†å¤æ‚ï¼‰

#### 1. å¤šé˜¶æ®µè¯¾ç¨‹å­¦ä¹ 
```python
# Stage 1: ç®€å•è½¨è¿¹ (100k steps)
- æ…¢é€Ÿåœ†å½¢è½¨è¿¹
- æ— å¹²æ‰°

# Stage 2: ä¸­ç­‰éš¾åº¦ (300k steps)
- å¿«é€Ÿè½¨è¿¹
- å°å¹…åº¦å¹²æ‰°

# Stage 3: å›°éš¾åœºæ™¯ (600k steps)  
- å¤æ‚è½¨è¿¹
- å¤§å¹…åº¦å¹²æ‰°
- è´Ÿè½½å˜åŒ–
```

#### 2. é’ˆå¯¹ä¸åŒæœºå™¨äººçš„ä¸“é—¨è®­ç»ƒ
```python
# Franka Panda: é«˜ç²¾åº¦æ“ä½œä»»åŠ¡
- é‡ç‚¹ä¼˜åŒ–J2 (shoulder pitch)
- å¢åŠ æœ«ç«¯ç²¾åº¦å¥–åŠ±

# Laikago: ç¨³å®šæ€§å’Œé²æ£’æ€§
- é‡ç‚¹ä¼˜åŒ–é«‹å…³èŠ‚ (J1, J4, J7, J10)
- å¢åŠ ç¨³å®šæ€§å¥–åŠ±
```

#### 3. ä½¿ç”¨æ›´å…ˆè¿›çš„RLç®—æ³•
```python
# ä»PPOå‡çº§åˆ°:
- SAC (Soft Actor-Critic) - æ›´ç¨³å®š
- TD3 (Twin Delayed DDPG) - æ›´é«˜æ•ˆ
- TRPO (Trust Region Policy Optimization) - æ›´å¯é 
```

**é¢„æœŸæ”¹å–„**: å¯è¾¾åˆ°Franka +40-50%, Laikago +15-20%

---

## ğŸ“‹ å®æ–½å»ºè®®ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

### ä¼˜å…ˆçº§1: ç«‹å³å®æ–½ï¼ˆ1-2å¤©ï¼‰ â­â­â­
1. âœ… å°†training stepsä»200kå¢åŠ åˆ°1M
2. âœ… ä¼˜åŒ–PPOè¶…å‚æ•°ï¼ˆn_steps=2048, batch_size=2048ï¼‰
3. âœ… å¢åŠ n_envsåˆ°8
4. âœ… é™ä½learning_rateåˆ°1e-4

**é¢„æœŸæˆæœ¬**: 10-20å°æ—¶GPUæ—¶é—´  
**é¢„æœŸå›æŠ¥**: Franka +20-30%, Laikago +5-10%

### ä¼˜å…ˆçº§2: çŸ­æœŸæ”¹è¿›ï¼ˆ3-5å¤©ï¼‰ â­â­
1. å®æ–½æ”¹è¿›çš„å¥–åŠ±å‡½æ•°
2. æ·»åŠ tensorboardç›‘æ§
3. å®æ–½early stoppingï¼ˆå¦‚æœæ€§èƒ½ä¸å†æ”¹å–„ï¼‰

**é¢„æœŸé¢å¤–æ”¹å–„**: +5-10%

### ä¼˜å…ˆçº§3: é•¿æœŸä¼˜åŒ–ï¼ˆ1-2å‘¨ï¼‰ â­
1. å®æ–½è¯¾ç¨‹å­¦ä¹ 
2. é’ˆå¯¹ä¸åŒæœºå™¨äººä¸“é—¨è®­ç»ƒ
3. å°è¯•ä¸åŒRLç®—æ³•

**é¢„æœŸé¢å¤–æ”¹å–„**: +10-20%

---

## ğŸ”§ å…·ä½“å®æ–½ä»£ç 

### ä¿®æ”¹train_meta_rl_combined.py:

```python
# 1. å¢åŠ è®­ç»ƒæ­¥æ•°å’Œä¼˜åŒ–è¶…å‚æ•°
model = PPO(
    'MlpPolicy', 
    env,
    learning_rate=1e-4,      # é™ä½å­¦ä¹ ç‡
    n_steps=2048,            # å¢åŠ steps
    batch_size=2048,         # å¢åŠ batch size
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.02,           # å¢åŠ æ¢ç´¢
    vf_coef=0.5,
    max_grad_norm=0.5,
    verbose=1,
    tensorboard_log="./logs/tensorboard/"
)

# 2. è®­ç»ƒ1Mæ­¥ï¼ˆä»200kå¢åŠ ï¼‰
model.learn(
    total_timesteps=1_000_000,  # å…³é”®ï¼š5å€è®­ç»ƒé‡
    callback=callback,
    log_interval=10,
    tb_log_name=f"meta_rl_{robot_name}_improved"
)
```

### ä¿®æ”¹meta_rl_combined_env.pyæ·»åŠ æ”¹è¿›çš„å¥–åŠ±å‡½æ•°:

```python
def step(self, action):
    # ... ç°æœ‰ä»£ç  ...
    
    # ä½¿ç”¨æ”¹è¿›çš„å¥–åŠ±å‡½æ•°
    reward = self._compute_reward_improved(
        error, qd, action, 
        self.prev_error if hasattr(self, 'prev_error') else None
    )
    self.prev_error = error  # ä¿å­˜ç”¨äºä¸‹ä¸€æ­¥
    
    # ... å…¶ä½™ä»£ç  ...

def _compute_reward_improved(self, error, qd, action, prev_error):
    """è§ä¸Šé¢çš„è¯¦ç»†å®ç°"""
    # ... (å¤åˆ¶ä¸Šé¢çš„ä»£ç ) ...
```

---

## ğŸ“Š é¢„æœŸç»“æœå¯¹æ¯”

| åœºæ™¯ | å½“å‰æ€§èƒ½ | ä¼˜å…ˆçº§1å | ä¼˜å…ˆçº§1+2å | ä¼˜å…ˆçº§1+2+3å |
|------|---------|-----------|-------------|---------------|
| **Franka Overall** | +10.9% | **+25%** | **+30%** | **+40%** |
| **Franka J2** | +72.6% | **+75%** | **+80%** | **+85%** |
| **Laikago Overall** | +0.1% | **+7%** | **+12%** | **+18%** |
| **è®­ç»ƒæ—¶é—´** | 2.5h | 15h | 20h | 30h |

---

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **GPUå†…å­˜**: å¦‚æœå¢åŠ n_envsåˆ°16ï¼Œå¯èƒ½éœ€è¦æ›´å¤šGPUå†…å­˜
2. **è®­ç»ƒæ—¶é—´**: 1M stepså¤§çº¦éœ€è¦10-15å°æ—¶ï¼ˆå–å†³äºç¡¬ä»¶ï¼‰
3. **è¿‡æ‹Ÿåˆé£é™©**: è®­ç»ƒå¤ªä¹…å¯èƒ½è¿‡æ‹Ÿåˆï¼Œéœ€è¦ç›‘æ§éªŒè¯æ€§èƒ½
4. **è¶…å‚æ•°è°ƒä¼˜**: å¯èƒ½éœ€è¦å¤šæ¬¡å®éªŒæ‰¾åˆ°æœ€ä½³è¶…å‚æ•°

---

## ğŸ’¡ æ€»ç»“

**æœ€å¿«è§æ•ˆçš„æ–¹æ³•**: 
1. å°†training stepsä»200kå¢åŠ åˆ°1M (**æœ€å…³é”®ï¼**)
2. ä¼˜åŒ–PPOè¶…å‚æ•°ï¼ˆn_steps=2048, batch_size=2048ï¼‰
3. é™ä½learning rateåˆ°1e-4

**é¢„æœŸæ—¶é—´**: 10-15å°æ—¶è®­ç»ƒ  
**é¢„æœŸæ•ˆæœ**: Franka +20-30%, Laikago +5-10%

**æ‚¨çš„æ•°æ®"ä¸å¥½çœ‹"çš„æ ¹æœ¬åŸå› **: RLè®­ç»ƒä¸¥é‡ä¸è¶³ï¼ˆä»…ç”¨äº†æ ‡å‡†è®­ç»ƒé‡çš„1/5-1/10ï¼‰

**å¥½æ¶ˆæ¯**: è¿™æ˜¯æœ€å®¹æ˜“è§£å†³çš„é—®é¢˜ï¼åªéœ€è¦å¢åŠ è®­ç»ƒæ—¶é—´å’Œä¼˜åŒ–è¶…å‚æ•°å³å¯ã€‚

