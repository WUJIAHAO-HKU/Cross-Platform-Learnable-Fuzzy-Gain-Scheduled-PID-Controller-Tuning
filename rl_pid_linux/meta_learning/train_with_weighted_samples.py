#!/usr/bin/env python3
"""
ä½¿ç”¨åŠ æƒæ ·æœ¬è®­ç»ƒå…ƒå­¦ä¹ PIDç½‘ç»œ
æ ¹æ®ä¼˜åŒ–è¯¯å·®è‡ªåŠ¨åˆ†é…æ ·æœ¬æƒé‡
"""

import json
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split


# ============================================================================
# SimplePIDPredictorï¼ˆä¸ä¹‹å‰ä¿æŒä¸€è‡´ï¼‰
# ============================================================================
class SimplePIDPredictor(nn.Module):
    """ç®€å•çš„MLPé¢„æµ‹å•ç»„PIDå‚æ•°"""
    def __init__(self, input_dim=4, hidden_dim=64, output_dim=3):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.Softplus()
        )
    
    def forward(self, x):
        return self.network(x)


# ============================================================================
# åŠ æƒæŸå¤±å‡½æ•°
# ============================================================================
class WeightedMSELoss(nn.Module):
    """åŠ æƒå‡æ–¹è¯¯å·®æŸå¤±"""
    def __init__(self):
        super().__init__()
    
    def forward(self, pred, target, weights):
        """
        Args:
            pred: é¢„æµ‹å€¼ (N, 3)
            target: çœŸå®å€¼ (N, 3)
            weights: æ ·æœ¬æƒé‡ (N,)
        """
        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„MSE
        mse = ((pred - target) ** 2).mean(dim=1)  # (N,)
        
        # åŠ æƒå¹³å‡
        weighted_mse = (mse * weights).sum() / weights.sum()
        
        return weighted_mse


# ============================================================================
# æ ·æœ¬æƒé‡è®¡ç®—
# ============================================================================
def compute_sample_weights(optimization_errors, weight_strategy='threshold'):
    """
    è®¡ç®—æ ·æœ¬æƒé‡
    
    Args:
        optimization_errors: ä¼˜åŒ–è¯¯å·®åˆ—è¡¨ï¼ˆåº¦æ•°ï¼‰
        weight_strategy: æƒé‡ç­–ç•¥
            - 'inverse': w = 1 / (1 + error/5) - ä¸¥æ ¼åæ¯”
            - 'exponential': w = exp(-error / 15) - ä¸¥æ ¼æŒ‡æ•°è¡°å‡
            - 'threshold': ä¸‰æ¡£æƒé‡ <20Â°â†’1.0, 20-35Â°â†’0.5, â‰¥35Â°â†’0.05
            - 'strict': åªç”¨è¯¯å·®<25Â°çš„æ ·æœ¬ï¼Œå…¶ä½™æƒé‡0
    
    Returns:
        weights: å½’ä¸€åŒ–çš„æƒé‡
    """
    errors = np.array(optimization_errors)
    
    if weight_strategy == 'inverse':
        # åæ¯”æƒé‡ï¼šè¯¯å·®è¶Šå°ï¼Œæƒé‡è¶Šå¤§ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        weights = 1.0 / (1.0 + errors / 5.0)  # é™¤ä»¥5ç¼©æ”¾ï¼ˆä¹‹å‰10ï¼Œç°åœ¨æ›´ä¸¥æ ¼ï¼‰
    
    elif weight_strategy == 'exponential':
        # æŒ‡æ•°æƒé‡ï¼šæ›´æ¿€è¿›åœ°é™ä½å¤§è¯¯å·®æ ·æœ¬æƒé‡ï¼ˆæ›´ä¸¥æ ¼ï¼‰
        weights = np.exp(-errors / 15.0)  # 15æ¯”ä¹‹å‰çš„20æ›´ä¸¥æ ¼
    
    elif weight_strategy == 'threshold':
        # é˜ˆå€¼æƒé‡ï¼šè¯¯å·®è¿‡å¤§çš„æ ·æœ¬é™æƒï¼ˆæ›´ä¸¥æ ¼ï¼‰
        threshold_high_quality = 20.0  # é«˜è´¨é‡é˜ˆå€¼
        threshold_acceptable = 35.0     # å¯æ¥å—é˜ˆå€¼
        
        # ä¸‰æ¡£æƒé‡ï¼šä¼˜ç§€(1.0), è‰¯å¥½(0.5), å·®(0.05)
        weights = np.where(errors < threshold_high_quality, 1.0,
                  np.where(errors < threshold_acceptable, 0.5, 0.05))
    
    elif weight_strategy == 'strict':
        # æœ€ä¸¥æ ¼ï¼šåªç”¨é«˜è´¨é‡æ ·æœ¬ï¼Œå…¶ä½™å®Œå…¨æ’é™¤
        strict_threshold = 25.0
        weights = np.where(errors < strict_threshold, 1.0, 0.0)
        
        n_excluded = (weights == 0).sum()
        print(f"   âš ï¸  strictæ¨¡å¼ï¼šæ’é™¤{n_excluded}ä¸ªæ ·æœ¬ï¼ˆè¯¯å·®â‰¥{strict_threshold}Â°ï¼‰")
    
    else:
        raise ValueError(f"Unknown weight strategy: {weight_strategy}")
    
    # å½’ä¸€åŒ–ï¼ˆä¿æŒæ€»æƒé‡=æ ·æœ¬æ•°ï¼‰
    weights = weights / weights.mean()
    
    return weights


# ============================================================================
# æ•°æ®åŠ è½½
# ============================================================================
def load_optimized_data(json_path):
    """åŠ è½½ä¼˜åŒ–åçš„æ•°æ®"""
    with open(json_path, 'r') as f:
        data = json.load(f)
    
    print(f"ğŸ“¦ åŠ è½½æ•°æ®: {len(data)}ä¸ªæ ·æœ¬")
    
    # æå–ç‰¹å¾å’Œæ ‡ç­¾
    features_list = []
    pid_list = []
    errors_list = []
    types = []
    
    for sample in data:
        # ä½¿ç”¨ç®€åŒ–çš„4ç»´ç‰¹å¾
        features = sample['features']
        feature_vec = [
            features['dof'],
            features['total_mass'],
            features['max_reach'],
            features['payload_mass']
        ]
        
        pid = sample['optimal_pid']
        pid_vec = [pid['kp'], pid['ki'], pid['kd']]
        
        # è·å–ä¼˜åŒ–è¯¯å·®ï¼ˆè™šæ‹Ÿæ ·æœ¬ï¼‰æˆ–0ï¼ˆçœŸå®æ ·æœ¬ï¼‰
        error = sample.get('optimization_error_deg', 0.0)
        
        features_list.append(feature_vec)
        pid_list.append(pid_vec)
        errors_list.append(error)
        types.append(sample['type'])
    
    X = np.array(features_list, dtype=np.float32)
    y = np.array(pid_list, dtype=np.float32)
    errors = np.array(errors_list, dtype=np.float32)
    
    print(f"   ç‰¹å¾å½¢çŠ¶: {X.shape}")
    print(f"   æ ‡ç­¾å½¢çŠ¶: {y.shape}")
    
    return X, y, errors, types, data


def normalize_data(X_train, X_test, y_train, y_test):
    """æ ‡å‡†åŒ–æ•°æ®"""
    # ç‰¹å¾æ ‡å‡†åŒ–
    X_mean = X_train.mean(axis=0)
    X_std = X_train.std(axis=0) + 1e-8
    X_train_norm = (X_train - X_mean) / X_std
    X_test_norm = (X_test - X_mean) / X_std
    
    # PIDæ ‡å‡†åŒ–ï¼ˆlog scaleï¼‰
    y_train_log = np.log(y_train + 1e-8)
    y_test_log = np.log(y_test + 1e-8)
    
    y_mean = y_train_log.mean(axis=0)
    y_std = y_train_log.std(axis=0) + 1e-8
    y_train_norm = (y_train_log - y_mean) / y_std
    y_test_norm = (y_test_log - y_mean) / y_std
    
    return X_train_norm, X_test_norm, y_train_norm, y_test_norm, X_mean, X_std, y_mean, y_std


# ============================================================================
# è®­ç»ƒå‡½æ•°
# ============================================================================
def train_meta_pid_weighted(X_train, y_train, weights_train, X_val, y_val, weights_val, epochs=500, lr=1e-3):
    """è®­ç»ƒåŠ æƒå…ƒå­¦ä¹ PIDç½‘ç»œ"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºæ¨¡å‹
    model = SimplePIDPredictor(input_dim=4, hidden_dim=64, output_dim=3).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    criterion = WeightedMSELoss()
    
    # è½¬æ¢ä¸ºTensor
    X_train_t = torch.FloatTensor(X_train).to(device)
    y_train_t = torch.FloatTensor(y_train).to(device)
    weights_train_t = torch.FloatTensor(weights_train).to(device)
    
    X_val_t = torch.FloatTensor(X_val).to(device)
    y_val_t = torch.FloatTensor(y_val).to(device)
    weights_val_t = torch.FloatTensor(weights_val).to(device)
    
    # è®­ç»ƒå†å²
    history = {'train_loss': [], 'val_loss': [], 'weighted_val_loss': []}
    
    best_val_loss = float('inf')
    patience = 50
    patience_counter = 0
    
    print(f"\nğŸš€ å¼€å§‹åŠ æƒè®­ç»ƒ... (epochs={epochs})")
    print(f"   è®­ç»ƒæ ·æœ¬æƒé‡èŒƒå›´: [{weights_train.min():.3f}, {weights_train.max():.3f}]")
    print(f"   è®­ç»ƒæ ·æœ¬å¹³å‡æƒé‡: {weights_train.mean():.3f}")
    
    for epoch in range(epochs):
        # è®­ç»ƒ
        model.train()
        optimizer.zero_grad()
        pred = model(X_train_t)
        loss = criterion(pred, y_train_t, weights_train_t)
        loss.backward()
        optimizer.step()
        
        # éªŒè¯ï¼ˆåŠ æƒï¼‰
        model.eval()
        with torch.no_grad():
            val_pred = model(X_val_t)
            val_loss_weighted = criterion(val_pred, y_val_t, weights_val_t)
            # ä¹Ÿè®¡ç®—æ— æƒé‡æŸå¤±ç”¨äºç›‘æ§
            val_loss_unweighted = ((val_pred - y_val_t) ** 2).mean()
        
        history['train_loss'].append(loss.item())
        history['weighted_val_loss'].append(val_loss_weighted.item())
        history['val_loss'].append(val_loss_unweighted.item())
        
        # Early stoppingï¼ˆåŸºäºåŠ æƒéªŒè¯æŸå¤±ï¼‰
        if val_loss_weighted < best_val_loss:
            best_val_loss = val_loss_weighted
            patience_counter = 0
            best_model_state = model.state_dict()
        else:
            patience_counter += 1
        
        if (epoch + 1) % 50 == 0:
            print(f"Epoch {epoch+1}/{epochs} - "
                  f"Train Loss: {loss.item():.6f}, "
                  f"Val Loss (weighted): {val_loss_weighted.item():.6f}, "
                  f"Val Loss (raw): {val_loss_unweighted.item():.6f}")
        
        if patience_counter >= patience:
            print(f"â¹ï¸  Early stopping at epoch {epoch+1}")
            break
    
    # æ¢å¤æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_model_state)
    
    print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ä½³åŠ æƒéªŒè¯æŸå¤±: {best_val_loss:.6f}")
    
    return model, history


# ============================================================================
# è¯„ä¼°å‡½æ•°
# ============================================================================
def evaluate_weighted_model(model, X_test, y_test, errors_test, X_mean, X_std, y_mean, y_std):
    """è¯„ä¼°åŠ æƒæ¨¡å‹"""
    device = next(model.parameters()).device
    
    # æ ‡å‡†åŒ–æµ‹è¯•æ•°æ®
    X_test_norm = (X_test - X_mean) / X_std
    y_test_log = np.log(y_test + 1e-8)
    y_test_norm = (y_test_log - y_mean) / y_std
    
    # é¢„æµ‹
    model.eval()
    with torch.no_grad():
        X_test_t = torch.FloatTensor(X_test_norm).to(device)
        pred_norm = model(X_test_t).cpu().numpy()
    
    # åæ ‡å‡†åŒ–
    pred_log = pred_norm * y_std + y_mean
    pred = np.exp(pred_log)
    
    # è®¡ç®—è¯¯å·®
    abs_errors = np.abs(pred - y_test)
    
    # æŒ‰ä¼˜åŒ–è¯¯å·®åˆ†ç»„è¯„ä¼°
    low_error_mask = errors_test < 30
    high_error_mask = errors_test >= 30
    
    print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print(f"\nå…¨ä½“æ ·æœ¬ (n={len(X_test)}):")
    print(f"   Kp ç»å¯¹è¯¯å·®: {abs_errors[:, 0].mean():.4f}")
    print(f"   Ki ç»å¯¹è¯¯å·®: {abs_errors[:, 1].mean():.4f}")
    print(f"   Kd ç»å¯¹è¯¯å·®: {abs_errors[:, 2].mean():.4f}")
    print(f"   æ€»ä½“å¹³å‡: {abs_errors.mean():.4f}")
    
    if low_error_mask.any():
        print(f"\nä½ä¼˜åŒ–è¯¯å·®æ ·æœ¬ (ä¼˜åŒ–è¯¯å·®<30Â°, n={low_error_mask.sum()}):")
        print(f"   Kp ç»å¯¹è¯¯å·®: {abs_errors[low_error_mask, 0].mean():.4f}")
        print(f"   æ€»ä½“å¹³å‡: {abs_errors[low_error_mask].mean():.4f}")
    
    if high_error_mask.any():
        print(f"\né«˜ä¼˜åŒ–è¯¯å·®æ ·æœ¬ (ä¼˜åŒ–è¯¯å·®â‰¥30Â°, n={high_error_mask.sum()}):")
        print(f"   Kp ç»å¯¹è¯¯å·®: {abs_errors[high_error_mask, 0].mean():.4f}")
        print(f"   æ€»ä½“å¹³å‡: {abs_errors[high_error_mask].mean():.4f}")
    
    return abs_errors, pred


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================
def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    print("=" * 80)
    print("åŠ æƒå…ƒå­¦ä¹ PIDè®­ç»ƒ")
    print("=" * 80)
    
    # 1. åŠ è½½ä¼˜åŒ–åçš„æ•°æ®ï¼ˆè¿‡æ»¤ç‰ˆï¼šæ’é™¤Laikagoè™šæ‹Ÿæ ·æœ¬ï¼‰
    data_path = Path(__file__).parent / 'augmented_pid_data_filtered.json'
    print(f"ğŸ“ åŠ è½½æ•°æ®: {data_path.name}")
    X_full, y_full, errors_full, types, data_full = load_optimized_data(data_path)
    
    # 2. åˆ†ææ ·æœ¬åˆ†å¸ƒ
    print(f"\nğŸ“Š æ ·æœ¬ä¼˜åŒ–è¯¯å·®ç»Ÿè®¡:")
    print(f"   å¹³å‡: {errors_full.mean():.2f}Â°")
    print(f"   ä¸­ä½: {np.median(errors_full):.2f}Â°")
    print(f"   æœ€å°: {errors_full.min():.2f}Â°")
    print(f"   æœ€å¤§: {errors_full.max():.2f}Â°")
    print(f"   <10Â°: {(errors_full < 10).sum()} æ ·æœ¬")
    print(f"   10-30Â°: {((errors_full >= 10) & (errors_full < 30)).sum()} æ ·æœ¬")
    print(f"   30-50Â°: {((errors_full >= 30) & (errors_full < 50)).sum()} æ ·æœ¬")
    print(f"   â‰¥50Â°: {(errors_full >= 50).sum()} æ ·æœ¬")
    
    # 3. è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆæµ‹è¯•ä¸‰ç§ç­–ç•¥ï¼‰
    print(f"\nğŸ”§ æµ‹è¯•æƒé‡ç­–ç•¥:")
    for strategy in ['inverse', 'exponential', 'threshold']:
        weights = compute_sample_weights(errors_full, strategy)
        print(f"\n   {strategy}:")
        print(f"      æƒé‡èŒƒå›´: [{weights.min():.3f}, {weights.max():.3f}]")
        print(f"      å¹³å‡æƒé‡: {weights.mean():.3f}")
        print(f"      æƒé‡æ ‡å‡†å·®: {weights.std():.3f}")
        
        # æ˜¾ç¤ºä¸åŒè¯¯å·®æ®µçš„æƒé‡
        low_err = errors_full < 30
        high_err = errors_full >= 50
        if low_err.any():
            print(f"      ä½è¯¯å·®(<30Â°)å¹³å‡æƒé‡: {weights[low_err].mean():.3f}")
        if high_err.any():
            print(f"      é«˜è¯¯å·®(â‰¥50Â°)å¹³å‡æƒé‡: {weights[high_err].mean():.3f}")
    
    # 4. é€‰æ‹©æœ€ä½³ç­–ç•¥å¹¶è®­ç»ƒ
    weight_strategy = 'strict'  # ä½¿ç”¨strictç­–ç•¥ï¼ˆæœ€é«˜ç²¾åº¦è¦æ±‚ï¼‰
    
    print(f"\nâœ… é€‰æ‹©æƒé‡ç­–ç•¥: {weight_strategy}")
    print(f"   ç­–ç•¥è¯´æ˜:")
    if weight_strategy == 'strict':
        print(f"      è¯¯å·®<25Â°: æƒé‡1.0ï¼ˆä¿ç•™ï¼‰")
        print(f"      è¯¯å·®â‰¥25Â°: æƒé‡0.0ï¼ˆå®Œå…¨æ’é™¤ï¼‰")
        print(f"      ç›®æ ‡ï¼šåªç”¨é«˜è´¨é‡æ ·æœ¬ï¼Œç¡®ä¿æœ€é«˜é¢„æµ‹ç²¾åº¦")
    elif weight_strategy == 'threshold':
        print(f"      è¯¯å·®<20Â°: æƒé‡1.0ï¼ˆä¼˜ç§€ï¼‰")
        print(f"      è¯¯å·®20-35Â°: æƒé‡0.5ï¼ˆè‰¯å¥½ï¼‰")
        print(f"      è¯¯å·®â‰¥35Â°: æƒé‡0.05ï¼ˆå·®ï¼ŒåŸºæœ¬å¿½ç•¥ï¼‰")
    
    weights_full = compute_sample_weights(errors_full, weight_strategy)
    
    # 5. åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•é›†
    X_train, X_test, y_train, y_test, weights_train, weights_test, errors_train, errors_test, idx_train, idx_test = train_test_split(
        X_full, y_full, weights_full, errors_full, np.arange(len(X_full)),
        test_size=0.2, random_state=42
    )
    
    print(f"\nğŸ“¦ æ•°æ®åˆ’åˆ†:")
    print(f"   è®­ç»ƒæ ·æœ¬: {len(X_train)}")
    print(f"   æµ‹è¯•æ ·æœ¬: {len(X_test)}")
    
    # 6. æ ‡å‡†åŒ–
    X_train_norm, X_test_norm, y_train_norm, y_test_norm, X_mean, X_std, y_mean, y_std = \
        normalize_data(X_train, X_test, y_train, y_test)
    
    # 7. è®­ç»ƒ
    model, history = train_meta_pid_weighted(
        X_train_norm, y_train_norm, weights_train,
        X_test_norm, y_test_norm, weights_test,
        epochs=500, lr=1e-3
    )
    
    # 8. è¯„ä¼°
    test_data_subset = [data_full[i] for i in idx_test]
    abs_errors, pred = evaluate_weighted_model(
        model, X_test, y_test, errors_test,
        X_mean, X_std, y_mean, y_std
    )
    
    # 9. ä¿å­˜æ¨¡å‹
    model_save_path = Path(__file__).parent / 'meta_pid_weighted.pth'
    torch.save({
        'model_state_dict': model.state_dict(),
        'X_mean': X_mean,
        'X_std': X_std,
        'y_mean': y_mean,
        'y_std': y_std,
        'weight_strategy': weight_strategy,
        'test_error_mean': abs_errors.mean(),
    }, model_save_path)
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {model_save_path}")
    
    # 10. å¯è§†åŒ–
    plt.figure(figsize=(12, 5))
    
    # è®­ç»ƒæ›²çº¿
    plt.subplot(1, 2, 1)
    plt.plot(history['train_loss'], label='Train Loss', alpha=0.8)
    plt.plot(history['weighted_val_loss'], label='Val Loss (Weighted)', alpha=0.8)
    plt.plot(history['val_loss'], label='Val Loss (Raw)', alpha=0.8, linestyle='--')
    plt.xlabel('Epoch')
    plt.ylabel('MSE Loss')
    plt.title('Training Curve (Weighted)')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.yscale('log')
    
    # è¯¯å·®åˆ†å¸ƒ
    plt.subplot(1, 2, 2)
    plt.scatter(errors_test, abs_errors.mean(axis=1), alpha=0.5, s=30)
    plt.xlabel('Optimization Error (degrees)')
    plt.ylabel('Prediction Error')
    plt.title('Prediction vs Optimization Error')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plot_path = Path(__file__).parent / 'weighted_training_results.png'
    plt.savefig(plot_path, dpi=150, bbox_inches='tight')
    print(f"ğŸ“Š ç»“æœå›¾å·²ä¿å­˜: {plot_path}")
    
    print(f"\n{'='*80}")
    print(f"âœ… åŠ æƒè®­ç»ƒå®Œæˆï¼")
    print(f"{'='*80}")


if __name__ == '__main__':
    main()

