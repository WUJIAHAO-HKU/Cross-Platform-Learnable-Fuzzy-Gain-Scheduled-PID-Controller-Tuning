% RL Policy Network Architecture - PlotNeuralNet Style
% PPO Actor-Critic架构用于在线PID调整
% 编译: pdflatex rl_policy_network_architecture.tex

\documentclass[border=8pt, multi, tikz]{standalone}
\usepackage{import}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bm}

\usetikzlibrary{positioning, shapes.geometric, arrows.meta, calc, shadows, 3d, decorations.pathreplacing}

% 定义颜色
\definecolor{obscolor}{RGB}{52, 152, 219}        % 观测 - 蓝色
\definecolor{policycolor}{RGB}{231, 76, 60}      % 策略 - 红色
\definecolor{valuecolor}{RGB}{243, 156, 18}      % 价值 - 橙色
\definecolor{actioncolor}{RGB}{39, 174, 96}      % 动作 - 绿色
\definecolor{envcolor}{RGB}{155, 89, 182}        % 环境 - 紫色
\definecolor{rewardcolor}{RGB}{241, 196, 15}     % 奖励 - 黄色

% 3D立方体命令
\newcommand{\drawcube}[6]{
    \coordinate (A) at (#1, #2);
    \coordinate (B) at ($(A) + (#3, 0)$);
    \coordinate (C) at ($(B) + (0, #4)$);
    \coordinate (D) at ($(A) + (0, #4)$);
    
    \coordinate (E) at ($(A) + (#5*0.3, #5*0.3)$);
    \coordinate (F) at ($(B) + (#5*0.3, #5*0.3)$);
    \coordinate (G) at ($(C) + (#5*0.3, #5*0.3)$);
    \coordinate (H) at ($(D) + (#5*0.3, #5*0.3)$);
    
    \fill[#6!80, opacity=0.8] (E) -- (F) -- (G) -- (H) -- cycle;
    \fill[#6!90, opacity=0.8] (D) -- (H) -- (G) -- (C) -- cycle;
    \fill[#6!100, opacity=0.9] (A) -- (B) -- (C) -- (D) -- cycle;
    \fill[#6!95, opacity=0.8] (B) -- (F) -- (G) -- (C) -- cycle;
    \fill[#6!85, opacity=0.8] (E) -- (F) -- (B) -- (A) -- cycle;
    
    \draw[black, thick] (A) -- (B) -- (C) -- (D) -- cycle;
    \draw[black, thick] (E) -- (F) -- (G) -- (H) -- cycle;
    \draw[black, thick] (A) -- (E);
    \draw[black, thick] (B) -- (F);
    \draw[black, thick] (C) -- (G);
    \draw[black, thick] (D) -- (H);
}

\begin{document}
\begin{tikzpicture}[
    font=\sffamily\small,
    >=Stealth,
    node distance=0.5cm,
    arrow/.style={->, very thick, >=Stealth, shorten >=2pt, shorten <=2pt},
    dashedarrow/.style={->, very thick, dashed, >=Stealth},
]

%============================================================================
% Title
%============================================================================
\node[font=\sffamily\Large\bfseries] at (8, 9) 
    {RL Policy Network Architecture (PPO)};
\node[font=\sffamily\normalsize, text=gray] at (8, 8.3)
    {Online Adaptation of Meta-PID Parameters};

%============================================================================
% 观测输入 (Observation Space: 22D)
%============================================================================
\drawcube{0}{2}{1.5}{5}{1.2}{obscolor}
\node[text=white, font=\bfseries] at (0.75, 7.8) {Observation};
\node[text=white, font=\small] at (0.75, 7.3) {$s_t$ (22D)};

% 观测组成部分标注
\node[anchor=east, font=\scriptsize, align=right] at (-0.3, 6.0) 
    {$\bm{e}_q$ (position error)\\7D};
\node[anchor=east, font=\scriptsize, align=right] at (-0.3, 4.5)
    {$\dot{\bm{e}}_q$ (velocity error)\\7D};
\node[anchor=east, font=\scriptsize, align=right] at (-0.3, 3.0)
    {$\ddot{\bm{e}}_q$ (accel. error)\\7D};
\node[anchor=east, font=\scriptsize, align=right] at (-0.3, 2.3)
    {$t/T$ (time ratio)\\1D};

% 观测空间数学定义
\node[draw, rounded corners, fill=obscolor!15, text width=3.5cm, align=left,
      font=\tiny, drop shadow] at (0.75, 0.5)
{
    $\bm{s}_t = [\bm{e}_q; \dot{\bm{e}}_q; \ddot{\bm{e}}_q; t/T]$\\
    where:\\
    $\bm{e}_q = \bm{q}_{ref} - \bm{q}_{actual}$
};

%============================================================================
% Policy Network (Actor) - 2层MLP
%============================================================================

% Policy Layer 1 (256D)
\drawcube{3}{1.5}{1.5}{6}{1.2}{policycolor}
\node[text=white, font=\bfseries] at (3.75, 8.3) {Policy Layer 1};
\node[text=white, font=\small] at (3.75, 7.8) {256D};
\node[text=white, font=\scriptsize, align=center] at (3.75, 5.5)
    {Linear\\Tanh};

% 连接：Obs → Policy1
\draw[arrow, obscolor!70!black] (1.8, 4.5) -- (2.8, 4.5);

% Policy Layer 2 (256D)
\drawcube{6}{1.5}{1.5}{6}{1.2}{policycolor}
\node[text=white, font=\bfseries] at (6.75, 8.3) {Policy Layer 2};
\node[text=white, font=\small] at (6.75, 7.8) {256D};
\node[text=white, font=\scriptsize, align=center] at (6.75, 5.5)
    {Linear\\Tanh};

% 连接：Policy1 → Policy2
\draw[arrow, policycolor!70!black] (4.8, 4.5) -- (5.8, 4.5);

%============================================================================
% Action Output (2D: Delta Kp, Delta Kd)
%============================================================================
\drawcube{9.5}{3}{1}{2.5}{0.8}{actioncolor}
\node[text=white, font=\bfseries] at (10, 6.1) {Action};
\node[text=white, font=\small] at (10, 5.6) {$\bm{a}_t$ (2D)};

% 连接：Policy2 → Action
\draw[arrow, policycolor!70!black] (7.8, 4.5) -- (9.3, 4.5);

% Action组成标注
\node[anchor=west, font=\scriptsize, align=left] at (11.2, 4.5)
    {$\Delta K_p$ ratio\\$\Delta K_d$ ratio};

% Action空间定义
\node[draw, rounded corners, fill=actioncolor!15, text width=3.5cm, align=left,
      font=\tiny, drop shadow] at (10, 1.5)
{
    $\bm{a}_t = [\alpha_{K_p}, \alpha_{K_d}]$\\
    $\alpha \in [-0.2, +0.2]$\\[0.1cm]
    Update rule:\\
    $K_p^{new} = K_p^{meta}(1+\alpha_{K_p})$\\
    $K_d^{new} = K_d^{meta}(1+\alpha_{K_d})$
};

%============================================================================
% Value Network (Critic) - 并行分支
%============================================================================

% 从Obs分叉到Value Network
\draw[arrow, obscolor!70!black] (1.2, 3.5) -- (2.5, 0);

% Value Layer (256D)
\drawcube{3}{-2.5}{1.2}{3}{1}{valuecolor}
\node[text=white, font=\bfseries] at (3.6, 1.2) {Value Layer};
\node[text=white, font=\small] at (3.6, 0.7) {256D};
\node[text=white, font=\scriptsize] at (3.6, -0.5) {Linear+Tanh};

% Value Output (1D)
\drawcube{6}{-2}{1}{2}{0.7}{valuecolor}
\node[text=white, font=\bfseries] at (6.5, 0.6) {Value};
\node[text=white, font=\small] at (6.5, 0.1) {$V(s_t)$};

% 连接
\draw[arrow, valuecolor!70!black] (4.5, -1) -- (5.8, -1);

% Value标注
\node[anchor=west, font=\scriptsize] at (7.2, -1)
    {State Value};

%============================================================================
% Environment (PyBullet Simulation)
%============================================================================
\node[draw, ultra thick, rounded corners=10pt, fill=envcolor!20, 
      minimum width=4cm, minimum height=3cm, drop shadow] at (13, 4.5)
{
    \begin{minipage}{3.5cm}
        \centering
        \textbf{\large Environment}\\[0.2cm]
        \small PyBullet Simulation\\[0.1cm]
        \scriptsize • Apply $\bm{a}_t$ to PID\\
        \scriptsize • Execute trajectory\\
        \scriptsize • Measure tracking error
    \end{minipage}
};

% Action → Environment
\draw[arrow, actioncolor!70!black, line width=2pt] (11.2, 4.5) -- (11.8, 4.5);

%============================================================================
% Reward Signal
%============================================================================
\node[draw, ultra thick, rounded corners, fill=rewardcolor!30,
      minimum width=2.5cm, minimum height=1.5cm, drop shadow] at (13, 1)
{
    \begin{minipage}{2cm}
        \centering
        \textbf{Reward}\\[0.1cm]
        $r_t = -\|\bm{e}_q\|_2$\\
        $-\lambda_1\|\bm{a}_t\|_2$\\
        $-\lambda_2\|\Delta\bm{a}_t\|_2$
    \end{minipage}
};

% Environment → Reward
\draw[arrow, envcolor!70!black, line width=2pt] (13, 3) -- (13, 2);

%============================================================================
% 反馈循环：Reward → Next Observation
%============================================================================
\draw[dashedarrow, rewardcolor!70!black, line width=2pt] 
    (11.8, 1) -- (1, 1) -- (1, 2);
\node[font=\scriptsize, fill=white] at (6, 1) {Next State $s_{t+1}$};

%============================================================================
% PPO训练流程标注
%============================================================================
\node[draw, ultra thick, rounded corners, fill=yellow!25, text width=15cm, 
      align=center, font=\normalsize, drop shadow] at (7, -4)
{
    \textbf{PPO Training Objective:}\\[0.2cm]
    $\displaystyle \mathcal{L}^{CLIP}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t\right)\right]$\\[0.1cm]
    $\displaystyle \mathcal{L}^{VF}(\phi) = \mathbb{E}_t\left[(V_\phi(s_t) - V_t^{target})^2\right]$\\[0.1cm]
    $\displaystyle \mathcal{L}^{PPO} = \mathcal{L}^{CLIP} + c_1\mathcal{L}^{VF} - c_2 S[\pi_\theta](s_t)$\\[0.1cm]
    \small where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$, 
    $\hat{A}_t$ = advantage, $S[\pi]$ = entropy bonus
};

%============================================================================
% 右侧：训练超参数
%============================================================================
\node[draw, thick, rounded corners, fill=blue!10, text width=4cm, align=left,
      font=\footnotesize, drop shadow] at (15.5, -1)
{
    \textbf{PPO Hyperparameters:}\\
    • Learning Rate: $3\times10^{-4}$\\
    • Batch Size: 64\\
    • Epochs per Update: 10\\
    • Clip Range ($\epsilon$): 0.2\\
    • GAE Lambda: 0.95\\
    • Discount ($\gamma$): 0.99\\
    • Entropy Coef ($c_2$): 0.01\\
    • Value Coef ($c_1$): 0.5
};

%============================================================================
% 左侧：训练统计
%============================================================================
\node[draw, thick, rounded corners, fill=green!10, text width=4cm, align=left,
      font=\footnotesize, drop shadow] at (-1.5, -1)
{
    \textbf{Training Statistics:}\\
    • Total Timesteps: 200k\\
    • Training Time: 20 min\\
    • Episodes: $\sim$1000\\
    • Final Reward: -2.37\\
    • Success Rate: 98.5\%\\
    • Convergence: Epoch 800
};

%============================================================================
% 底部：Actor-Critic架构优势
%============================================================================
\node[draw, thick, rounded corners, fill=orange!15, text width=7cm, align=left,
      font=\scriptsize, drop shadow] at (7, -6.5)
{
    \textbf{Actor-Critic Architecture Benefits:}\\
    ✓ \textbf{Actor (Policy Net):} Learns optimal PID adjustment policy\\
    ✓ \textbf{Critic (Value Net):} Evaluates state quality for variance reduction\\
    ✓ \textbf{PPO:} Stable training with clipped objectives\\
    ✓ \textbf{On-Policy:} Directly learns from robot interaction\\
    ✓ \textbf{Sample Efficient:} 200k steps = 20 minutes
};

%============================================================================
% 顶部：与Meta-PID的集成标注
%============================================================================
\draw[dashedarrow, thick, blue!60] (-1, 9.5) -- (0.3, 7.5);
\node[draw, rounded corners, fill=blue!15, text width=4cm, align=center,
      font=\scriptsize, drop shadow] at (-2.5, 10.5)
{
    \textbf{Integration with Meta-PID}\\[0.1cm]
    1. Meta-PID predicts\\
    initial $K_p^{meta}, K_d^{meta}$\\[0.1cm]
    2. RL adapts online:\\
    $K_p^{final} = K_p^{meta}(1+\alpha_{K_p})$\\[0.1cm]
    3. Hierarchical control:\\
    \textbf{Meta-Learning + RL}
};

\end{tikzpicture}
\end{document}

