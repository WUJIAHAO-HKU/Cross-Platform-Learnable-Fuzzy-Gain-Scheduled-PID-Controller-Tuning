# 🚀 优化版RL训练脚本 - 使用指南

## ✅ 已完成的优化

### 超参数对比

| 参数 | 原始值 | 优化值 | 改进 |
|------|--------|--------|------|
| **total_timesteps** | 200,000 | **1,000,000** | 5倍训练量 ⭐⭐⭐ |
| **learning_rate** | 3e-4 | **1e-4** | 更稳定学习 ⭐⭐ |
| **n_steps** | 256 | **2048** | 8倍经验收集 ⭐⭐⭐ |
| **batch_size** | 64 | **256** | 4倍梯度稳定性 ⭐⭐ |
| **ent_coef** | 0.01 | **0.02** | 更多探索 ⭐ |
| **n_envs** | 8 | **8** | 保持 |

### 预期性能提升

| 机器人 | 原始性能 | 预期优化后 | 改善幅度 |
|--------|---------|------------|---------|
| **Franka Panda** | +10.9% | **+25-30%** | 2-3倍 |
| **Laikago** | +0.1% | **+7-12%** | 70-120倍 |

---

## 📝 训练命令

### 1. Franka Panda (推荐先训练这个)

```bash
cd /home/wujiahao/基于强化学习的模型预测控制动力学模型误差在线补偿方法研究/rl_pid_linux/meta_learning
conda activate rl_robot_env

# 使用默认配置 (1M步)
python train_meta_rl_combined.py franka_panda/panda.urdf

# 或指定步数
python train_meta_rl_combined.py franka_panda/panda.urdf 1000000 8

# 如果想快速测试（不推荐用于论文）
python train_meta_rl_combined.py franka_panda/panda.urdf 500000 8
```

**预计时间**: 3-5小时  
**预期结果**: Overall改善从+10.9%提升到+25-30%

### 2. Laikago (如果时间充足)

```bash
# 使用默认配置
python train_meta_rl_combined.py laikago/laikago.urdf

# Laikago可能需要更多步数
python train_meta_rl_combined.py laikago/laikago.urdf 1500000 8
```

**预计时间**: 4-6小时  
**预期结果**: Overall改善从+0.1%提升到+7-12%

### 3. 并行训练两个机器人（如果有多个GPU）

```bash
# 终端1: Franka
CUDA_VISIBLE_DEVICES=0 python train_meta_rl_combined.py franka_panda/panda.urdf 1000000 8

# 终端2: Laikago
CUDA_VISIBLE_DEVICES=1 python train_meta_rl_combined.py laikago/laikago.urdf 1500000 8
```

---

## 📊 监控训练进度

### 1. Tensorboard实时监控

```bash
# 打开新终端
cd /home/wujiahao/基于强化学习的模型预测控制动力学模型误差在线补偿方法研究/rl_pid_linux/meta_learning

# 监控Franka训练
tensorboard --logdir logs/meta_rl_panda/tensorboard --port 6006

# 监控Laikago训练
tensorboard --logdir logs/meta_rl_laikago/tensorboard --port 6007

# 然后在浏览器打开: http://localhost:6006
```

### 2. 关键指标

**训练良好的标志**:
- ✅ Episode reward 持续上升
- ✅ Value loss 逐渐下降
- ✅ Policy loss 稳定在较低水平
- ✅ Explained variance 增加到0.7+
- ✅ Entropy 缓慢下降（0.6 → 0.3）

**需要调整的标志**:
- ❌ Reward没有上升趋势
- ❌ Loss震荡剧烈
- ❌ Clip fraction > 0.3 (学习率太大)
- ❌ Entropy太快降到0 (探索不足)

---

## 🔧 训练后评估

### 1. 测试训练好的模型

```bash
# 测试Franka
python test_disturbance_scenarios.py \
  --robot franka_panda/panda.urdf \
  --model logs/meta_rl_panda/best_model/best_model \
  --n_episodes 10

# 测试Laikago
python test_disturbance_scenarios.py \
  --robot laikago/laikago.urdf \
  --model logs/meta_rl_laikago/best_model/best_model \
  --n_episodes 10
```

### 2. 重新生成所有图表

```bash
# 使用新的模型重新评估和生成图表
python generate_all_figures_unified.py
```

这将生成新的：
- `per_joint_error.png` (Figure 3)
- `Figure4_comprehensive_tracking_performance.png` (Figure 4)

---

## ⚙️ 高级配置

### 如果想尝试更激进的训练（追求极致性能）

```python
# 修改 train_meta_rl_combined.py 中的参数
total_timesteps = 2000000  # 2M步（约6-10小时）
n_envs = 16                # 16个并行环境（需要更多GPU内存）
```

### 如果GPU内存不足

```python
n_envs = 4  # 减少并行环境
```

### 如果想加快训练（牺牲性能）

```python
total_timesteps = 500000   # 500k步（约1.5-2.5小时）
n_steps = 1024             # 减少steps
```

---

## 📋 训练检查清单

开始训练前：
- [ ] 确认已激活conda环境: `conda activate rl_robot_env`
- [ ] 确认GPU可用: `python -c "import torch; print(torch.cuda.is_available())"`
- [ ] 确认Meta-PID模型存在: `ls meta_pid_augmented.pth`
- [ ] 清空旧的训练日志（可选）: `rm -rf logs/meta_rl_panda logs/meta_rl_laikago`

训练过程中：
- [ ] 监控GPU使用: `nvidia-smi`
- [ ] 监控Tensorboard
- [ ] 检查日志文件中有无错误
- [ ] 确保episode reward有上升趋势

训练完成后：
- [ ] 检查best_model是否保存
- [ ] 运行评估脚本测试性能
- [ ] 重新生成图表
- [ ] 对比训练前后的性能

---

## 🎯 预期时间线

| 任务 | 时间 | 说明 |
|------|------|------|
| **Franka训练** | 3-5小时 | 1M步，主要实验 |
| **Laikago训练** | 4-6小时 | 1.5M步，泛化验证 |
| **评估测试** | 30分钟 | 10个episode，5个场景 |
| **重新生成图表** | 30分钟 | 所有图表 |
| **总计** | **8-12小时** | 可过夜运行 |

---

## 💡 最佳实践

1. **先训练Franka** - 更快看到效果，验证配置
2. **使用nohup后台运行** - 避免终端关闭中断
   ```bash
   nohup python train_meta_rl_combined.py franka_panda/panda.urdf > franka_train.log 2>&1 &
   ```
3. **定期检查进度** - 每小时看一次Tensorboard
4. **保存中间检查点** - 脚本已自动配置（每20k步保存）
5. **对比原始模型** - 训练完成后与原200k步模型对比

---

## ⚠️ 故障排除

### 问题1: CUDA out of memory
**解决**: 减少n_envs到4或2

### 问题2: 训练速度很慢
**解决**: 
- 确认使用GPU: `device='cuda'`
- 检查GPU使用率: `nvidia-smi`
- 减少n_steps到1024

### 问题3: Reward不上升
**解决**: 
- 检查Meta-PID是否加载正确
- 尝试降低learning_rate到5e-5
- 增加训练步数

### 问题4: Loss震荡剧烈
**解决**:
- 降低learning_rate
- 增加batch_size
- 减少ent_coef

---

## 📞 需要帮助？

如果遇到问题，检查：
1. 训练日志: `logs/meta_rl_*/`
2. Tensorboard曲线
3. GPU状态: `nvidia-smi`
4. 进程状态: `ps aux | grep python`

记得训练前备份重要数据！🔒

