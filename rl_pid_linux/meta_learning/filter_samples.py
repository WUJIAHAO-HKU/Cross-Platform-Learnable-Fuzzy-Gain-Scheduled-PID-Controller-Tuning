#!/usr/bin/env python3
"""
æ•°æ®è¿‡æ»¤è„šæœ¬ï¼šç§»é™¤ä¼˜åŒ–è¯¯å·®è¿‡å¤§çš„ä¸å¯æ§æ ·æœ¬

ç”¨é€”ï¼š
- è¿‡æ»¤æ‰optimization_error > thresholdçš„æ ·æœ¬
- ä¿ç•™é«˜è´¨é‡å¯æ§æ ·æœ¬ç”¨äºå…ƒå­¦ä¹ è®­ç»ƒ
- è¾“å‡ºè¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯
"""

import json
import argparse
import numpy as np
from pathlib import Path


def filter_samples(input_file, output_file, error_threshold=30.0, min_samples_per_type=30):
    """
    è¿‡æ»¤ä¼˜åŒ–è¯¯å·®è¿‡å¤§çš„æ ·æœ¬
    
    Args:
        input_file: è¾“å…¥çš„ä¼˜åŒ–åæ•°æ®æ–‡ä»¶
        output_file: è¾“å‡ºçš„è¿‡æ»¤åæ•°æ®æ–‡ä»¶
        error_threshold: è¯¯å·®é˜ˆå€¼ï¼ˆåº¦ï¼‰ï¼Œé»˜è®¤30Â°
        min_samples_per_type: æ¯ç§æœºå™¨äººç±»å‹æœ€å°‘ä¿ç•™çš„æ ·æœ¬æ•°
    """
    
    print("="*80)
    print("æ•°æ®æ ·æœ¬è¿‡æ»¤")
    print("="*80)
    
    # åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®: {input_file}")
    with open(input_file, 'r') as f:
        data = json.load(f)
    
    print(f"   åŸå§‹æ ·æœ¬æ•°: {len(data)}")
    
    # æŒ‰æœºå™¨äººç±»å‹åˆ†ç»„ï¼ˆæ ¹æ®nameå­—æ®µæ¨æ–­ç±»å‹ï¼‰
    by_type = {}
    for sample in data:
        name = sample.get('name', 'unknown')
        # æ¨æ–­æœºå™¨äººç±»å‹
        if 'panda' in name.lower():
            robot_type = 'Panda'
        elif 'laikago' in name.lower():
            robot_type = 'Laikago'
        elif 'kuka' in name.lower() or 'model' in name.lower():
            robot_type = 'KUKA'
        else:
            robot_type = name
        
        if robot_type not in by_type:
            by_type[robot_type] = []
        by_type[robot_type].append(sample)
    
    print(f"\nğŸ“Š åŸå§‹æ ·æœ¬åˆ†å¸ƒ:")
    for robot_type, samples in by_type.items():
        # åªç»Ÿè®¡æœ‰ä¼˜åŒ–è¯¯å·®çš„æ ·æœ¬ï¼ˆè™šæ‹Ÿæ ·æœ¬ï¼‰
        errors = [s.get('optimization_error_deg', 0) for s in samples if 'optimization_error_deg' in s]
        if errors:
            print(f"   {robot_type}: {len(samples)}ä¸ªæ ·æœ¬, "
                  f"å¹³å‡è¯¯å·®={np.mean(errors):.2f}Â°, "
                  f"ä¸­ä½è¯¯å·®={np.median(errors):.2f}Â°")
        else:
            print(f"   {robot_type}: {len(samples)}ä¸ªæ ·æœ¬ (çœŸå®æœºå™¨äººï¼Œæ— ä¼˜åŒ–è¯¯å·®)")
    
    # è¿‡æ»¤é€»è¾‘
    print(f"\nğŸ” è¿‡æ»¤æ¡ä»¶:")
    print(f"   è¯¯å·®é˜ˆå€¼: {error_threshold}Â°")
    print(f"   æ¯ç±»æœ€å°‘ä¿ç•™: {min_samples_per_type}ä¸ª")
    
    filtered_data = []
    filter_stats = {}
    
    for robot_type, samples in by_type.items():
        # æŒ‰ä¼˜åŒ–è¯¯å·®æ’åºï¼ˆä»å°åˆ°å¤§ï¼‰ï¼ŒçœŸå®æ ·æœ¬ï¼ˆæ— è¯¯å·®å­—æ®µï¼‰æ’åœ¨æœ€å‰
        samples_sorted = sorted(samples, key=lambda x: x.get('optimization_error_deg', 0))
        
        # åº”ç”¨é˜ˆå€¼è¿‡æ»¤ï¼ˆä¿ç•™çœŸå®æ ·æœ¬å’Œè¯¯å·®å°äºé˜ˆå€¼çš„è™šæ‹Ÿæ ·æœ¬ï¼‰
        samples_passed = [s for s in samples_sorted 
                         if 'optimization_error_deg' not in s or s['optimization_error_deg'] <= error_threshold]
        
        # ç¡®ä¿è‡³å°‘ä¿ç•™min_samples_per_typeä¸ªæ ·æœ¬
        if len(samples_passed) < min_samples_per_type:
            print(f"   âš ï¸  {robot_type}: é€šè¿‡é˜ˆå€¼çš„æ ·æœ¬ä¸è¶³ ({len(samples_passed)}/{min_samples_per_type})")
            print(f"       å¼ºåˆ¶ä¿ç•™è¯¯å·®æœ€å°çš„{min_samples_per_type}ä¸ªæ ·æœ¬")
            samples_kept = samples_sorted[:min_samples_per_type]
        else:
            samples_kept = samples_passed
        
        # ç»Ÿè®¡
        original_count = len(samples)
        kept_count = len(samples_kept)
        removed_count = original_count - kept_count
        keep_rate = kept_count / original_count * 100
        
        # è®¡ç®—è¯¯å·®ç»Ÿè®¡ï¼ˆåªç»Ÿè®¡è™šæ‹Ÿæ ·æœ¬ï¼‰
        errors_before = [s['optimization_error_deg'] for s in samples if 'optimization_error_deg' in s]
        errors_after = [s['optimization_error_deg'] for s in samples_kept if 'optimization_error_deg' in s]
        
        filter_stats[robot_type] = {
            'original': original_count,
            'kept': kept_count,
            'removed': removed_count,
            'keep_rate': keep_rate,
            'avg_error_before': np.mean(errors_before) if errors_before else 0,
            'avg_error_after': np.mean(errors_after) if errors_after else 0,
            'max_error_after': max(errors_after) if errors_after else 0
        }
        
        filtered_data.extend(samples_kept)
    
    # æ‰“å°è¿‡æ»¤ç»Ÿè®¡
    print(f"\nğŸ“Š è¿‡æ»¤ç»“æœ:")
    print(f"{'æœºå™¨äººç±»å‹':<15} {'åŸå§‹':<8} {'ä¿ç•™':<8} {'ç§»é™¤':<8} {'ä¿ç•™ç‡':<10} {'å¹³å‡è¯¯å·®':<15}")
    print("-" * 80)
    
    for robot_type, stats in filter_stats.items():
        print(f"{robot_type:<15} "
              f"{stats['original']:<8} "
              f"{stats['kept']:<8} "
              f"{stats['removed']:<8} "
              f"{stats['keep_rate']:<10.1f}% "
              f"{stats['avg_error_before']:.2f}Â° â†’ {stats['avg_error_after']:.2f}Â°")
    
    # æ€»ä½“ç»Ÿè®¡
    original_total = len(data)
    kept_total = len(filtered_data)
    removed_total = original_total - kept_total
    overall_keep_rate = kept_total / original_total * 100
    
    print("-" * 80)
    print(f"{'æ€»è®¡':<15} "
          f"{original_total:<8} "
          f"{kept_total:<8} "
          f"{removed_total:<8} "
          f"{overall_keep_rate:<10.1f}%")
    
    # è´¨é‡æ”¹å–„ï¼ˆåªç»Ÿè®¡è™šæ‹Ÿæ ·æœ¬ï¼‰
    all_errors_before = [s['optimization_error_deg'] for s in data if 'optimization_error_deg' in s]
    all_errors_after = [s['optimization_error_deg'] for s in filtered_data if 'optimization_error_deg' in s]
    
    print(f"\nğŸ“ˆ è´¨é‡æå‡:")
    print(f"   å¹³å‡è¯¯å·®: {np.mean(all_errors_before):.2f}Â° â†’ {np.mean(all_errors_after):.2f}Â° "
          f"(æ”¹å–„ {(1 - np.mean(all_errors_after)/np.mean(all_errors_before))*100:.1f}%)")
    print(f"   ä¸­ä½è¯¯å·®: {np.median(all_errors_before):.2f}Â° â†’ {np.median(all_errors_after):.2f}Â°")
    print(f"   æœ€å¤§è¯¯å·®: {max(all_errors_before):.2f}Â° â†’ {max(all_errors_after):.2f}Â°")
    print(f"   æ ‡å‡†å·®: {np.std(all_errors_before):.2f}Â° â†’ {np.std(all_errors_after):.2f}Â°")
    
    # ä¿å­˜è¿‡æ»¤åçš„æ•°æ®
    print(f"\nğŸ’¾ ä¿å­˜è¿‡æ»¤åçš„æ•°æ®: {output_file}")
    with open(output_file, 'w') as f:
        json.dump(filtered_data, f, indent=2)
    
    print(f"\nâœ… è¿‡æ»¤å®Œæˆï¼")
    print(f"   åŸå§‹æ ·æœ¬: {original_total}")
    print(f"   ä¿ç•™æ ·æœ¬: {kept_total}")
    print(f"   ç§»é™¤æ ·æœ¬: {removed_total}")
    print(f"   ä¿ç•™ç‡: {overall_keep_rate:.1f}%")
    print(f"   å¹³å‡è¯¯å·®: {np.mean(all_errors_after):.2f}Â°")
    
    # å»ºè®®
    print(f"\nğŸ¯ å»ºè®®:")
    if overall_keep_rate < 70:
        print(f"   âš ï¸  ä¿ç•™ç‡è¾ƒä½ ({overall_keep_rate:.1f}%)")
        print(f"   å»ºè®®: æ£€æŸ¥æ•°æ®å¢å¼ºçš„æ‰°åŠ¨èŒƒå›´æ˜¯å¦è¿‡å¤§")
    elif overall_keep_rate > 95:
        print(f"   ğŸ’¡ ä¿ç•™ç‡å¾ˆé«˜ ({overall_keep_rate:.1f}%)")
        print(f"   å»ºè®®: å¯ä»¥é€‚å½“é™ä½é˜ˆå€¼ä»¥è¿›ä¸€æ­¥æå‡è´¨é‡")
    else:
        print(f"   âœ… ä¿ç•™ç‡é€‚ä¸­ ({overall_keep_rate:.1f}%)")
        print(f"   å»ºè®®: æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒ")
    
    if np.mean(all_errors_after) > 20:
        print(f"   âš ï¸  å¹³å‡è¯¯å·®ä»ç„¶è¾ƒé«˜ ({np.mean(all_errors_after):.2f}Â°)")
        print(f"   å»ºè®®: è€ƒè™‘è¿›ä¸€æ­¥é™ä½é˜ˆå€¼æˆ–è°ƒæ•´æ•°æ®å¢å¼ºç­–ç•¥")
    
    print(f"\nğŸ“– ä¸‹ä¸€æ­¥:")
    print(f"   ä½¿ç”¨è¿‡æ»¤åçš„æ•°æ®è®­ç»ƒå…ƒå­¦ä¹ ç½‘ç»œ:")
    print(f"   python train_meta_learning.py --data {output_file}")
    
    return filtered_data, filter_stats


def analyze_removed_samples(input_file, output_file, error_threshold=30.0):
    """åˆ†æè¢«ç§»é™¤çš„æ ·æœ¬ç‰¹å¾"""
    
    with open(input_file, 'r') as f:
        data = json.load(f)
    
    # åˆ†ç¦»ä¿ç•™å’Œç§»é™¤çš„æ ·æœ¬ï¼ˆåªè€ƒè™‘è™šæ‹Ÿæ ·æœ¬ï¼‰
    kept_samples = [s for s in data 
                   if 'optimization_error_deg' not in s or s['optimization_error_deg'] <= error_threshold]
    removed_samples = [s for s in data 
                      if 'optimization_error_deg' in s and s['optimization_error_deg'] > error_threshold]
    
    if len(removed_samples) == 0:
        print("\nâœ… æ²¡æœ‰æ ·æœ¬è¢«ç§»é™¤")
        return
    
    print(f"\nğŸ”¬ è¢«ç§»é™¤æ ·æœ¬åˆ†æ (n={len(removed_samples)}):")
    
    # æŒ‰æœºå™¨äººç±»å‹ç»Ÿè®¡
    removed_by_type = {}
    for sample in removed_samples:
        name = sample.get('name', 'unknown')
        # æ¨æ–­æœºå™¨äººç±»å‹
        if 'panda' in name.lower():
            robot_type = 'Panda'
        elif 'laikago' in name.lower():
            robot_type = 'Laikago'
        elif 'kuka' in name.lower() or 'model' in name.lower():
            robot_type = 'KUKA'
        else:
            robot_type = name
        
        if robot_type not in removed_by_type:
            removed_by_type[robot_type] = []
        removed_by_type[robot_type].append(sample)
    
    for robot_type, samples in removed_by_type.items():
        errors = [s['optimization_error_deg'] for s in samples]
        print(f"\n   {robot_type} (ç§»é™¤{len(samples)}ä¸ª):")
        print(f"     è¯¯å·®èŒƒå›´: {min(errors):.2f}Â° - {max(errors):.2f}Â°")
        print(f"     å¹³å‡è¯¯å·®: {np.mean(errors):.2f}Â°")
        
        # åˆ†æå‚æ•°ç‰¹å¾
        if 'augmentation_params' in samples[0]:
            mass_scales = [s['augmentation_params'].get('mass_scale', 1.0) for s in samples]
            inertia_scales = [s['augmentation_params'].get('inertia_scale', 1.0) for s in samples]
            
            print(f"     è´¨é‡ç¼©æ”¾: {np.mean(mass_scales):.2f} Â± {np.std(mass_scales):.2f}")
            print(f"     æƒ¯æ€§ç¼©æ”¾: {np.mean(inertia_scales):.2f} Â± {np.std(inertia_scales):.2f}")


def main():
    parser = argparse.ArgumentParser(description='è¿‡æ»¤ä¸å¯æ§çš„è™šæ‹Ÿæ ·æœ¬')
    parser.add_argument('--input', default='augmented_pid_data_optimized.json',
                        help='è¾“å…¥çš„ä¼˜åŒ–åæ•°æ®æ–‡ä»¶')
    parser.add_argument('--output', default='augmented_pid_data_filtered.json',
                        help='è¾“å‡ºçš„è¿‡æ»¤åæ•°æ®æ–‡ä»¶')
    parser.add_argument('--error_threshold', type=float, default=30.0,
                        help='ä¼˜åŒ–è¯¯å·®é˜ˆå€¼ï¼ˆåº¦ï¼‰ï¼Œé»˜è®¤30Â°')
    parser.add_argument('--min_samples_per_type', type=int, default=30,
                        help='æ¯ç§æœºå™¨äººç±»å‹æœ€å°‘ä¿ç•™çš„æ ·æœ¬æ•°ï¼Œé»˜è®¤30')
    parser.add_argument('--analyze', action='store_true',
                        help='åˆ†æè¢«ç§»é™¤æ ·æœ¬çš„ç‰¹å¾')
    
    args = parser.parse_args()
    
    # æ‰§è¡Œè¿‡æ»¤
    filtered_data, filter_stats = filter_samples(
        args.input,
        args.output,
        args.error_threshold,
        args.min_samples_per_type
    )
    
    # å¯é€‰ï¼šåˆ†æè¢«ç§»é™¤çš„æ ·æœ¬
    if args.analyze:
        analyze_removed_samples(args.input, args.output, args.error_threshold)


if __name__ == '__main__':
    main()

