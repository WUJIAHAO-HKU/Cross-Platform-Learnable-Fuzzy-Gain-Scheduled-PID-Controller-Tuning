# Linuxç¯å¢ƒå®æ–½è·¯çº¿å›¾ - RL+PIDè®ºæ–‡å‘è¡¨è®¡åˆ’

> **ç›®æ ‡**: åœ¨Linuxç¯å¢ƒä¸‹å®ç°ç³»ç»ŸåŒ–çš„RL+PIDç ”ç©¶ï¼Œå‘è¡¨é«˜è´¨é‡è®ºæ–‡
> **æ—¶é—´æ¡†æ¶**: 6-8å‘¨
> **ç›®æ ‡æœŸåˆŠ**: IEEE RAL, Control Engineering Practice, Robotics and Autonomous Systems

---

## ğŸ¯ æ ¸å¿ƒç­–ç•¥è°ƒæ•´

### ä»MATLABåˆ°Linuxçš„å…³é”®æ”¹è¿›

1. **æ›´ç¨³å®šçš„è®­ç»ƒç¯å¢ƒ**: PyBulletæä¾›ç¡®å®šæ€§ä»¿çœŸ
2. **æ›´å¥½çš„å¯è§†åŒ–**: å®æ—¶3Då¯è§†åŒ– + TensorBoardç›‘æ§
3. **æ›´çµæ´»çš„éƒ¨ç½²**: æ˜“äºè¿ç§»åˆ°Gazebo/ROS
4. **æ›´å¼ºçš„å¯å¤ç°æ€§**: Dockerå®¹å™¨ + é…ç½®ç®¡ç†

### è®ºæ–‡å‘è¡¨ç­–ç•¥

| æœŸåˆŠå±‚çº§ | ç›®æ ‡æœŸåˆŠ | æœ€ä½è¦æ±‚ | ç†æƒ³æˆæœ |
|---------|---------|---------|---------|
| **Tier 1** | IEEE RAL, Automatica | 25åœºæ™¯ï¼Œ50%æ”¹è¿›ï¼Œç†è®ºè¯æ˜ | éœ€è¦ç¨³å®šæ€§åˆ†æ |
| **Tier 2** | Control Engineering Practice | 10åœºæ™¯ï¼Œ40%æ”¹è¿› | **æ¨èé¦–æŠ•** |
| **Tier 3** | Robotics and Autonomous Systems | 5åœºæ™¯ï¼Œ30%æ”¹è¿› | ä¿åº•é€‰é¡¹ |

**å»ºè®®**: å…ˆèšç„¦Tier 2ï¼ˆCEPï¼‰ï¼Œå®ƒæ›´æ³¨é‡å®ç”¨æ€§ï¼Œå¯¹ç†è®ºè¯æ˜è¦æ±‚è¾ƒä½ï¼Œéå¸¸é€‚åˆRL+PIDè¿™ç±»å®ç”¨æ–¹æ³•ã€‚

---

## ğŸ“… è¯¦ç»†å®æ–½è®¡åˆ’

### é˜¶æ®µ1: Linuxç¯å¢ƒæ­å»ºï¼ˆWeek 1, 3-5å¤©ï¼‰

#### 1.1 ç³»ç»Ÿä¾èµ–å®‰è£…

```bash
# åˆ›å»ºç‹¬ç«‹condaç¯å¢ƒ
conda create -n rl_robot python=3.8 -y
conda activate rl_robot

# æ ¸å¿ƒä¾èµ–
pip install torch==1.13.1 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
pip install pybullet==3.2.5
pip install gym==0.21.0
pip install stable-baselines3==1.7.0
pip install numpy scipy matplotlib pandas
pip install tensorboard wandb  # å¯é€‰ï¼šåœ¨çº¿ç›‘æ§

# æ•°æ®å¤„ç†å’Œå¯è§†åŒ–
pip install seaborn scikit-learn
pip install opencv-python imageio

# ä¿å­˜ä¾èµ–åˆ—è¡¨
pip freeze > requirements.txt
```

#### 1.2 éªŒè¯PyBulletå®‰è£…

**æµ‹è¯•è„šæœ¬**: `tests/test_pybullet_franka.py`
- [ ] åŠ è½½Franka Panda URDF
- [ ] éªŒè¯7ä¸ªå…³èŠ‚å¯æ§
- [ ] æµ‹è¯•GUIå’Œæ— å¤´æ¨¡å¼
- [ ] éªŒè¯ç‰©ç†ä»¿çœŸç¨³å®šæ€§

#### 1.3 é¡¹ç›®ç»“æ„åˆ›å»º

```
rl_pid_linux/
â”œâ”€â”€ configs/               # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ robot_config.yaml
â”‚   â”œâ”€â”€ pid_config.yaml
â”‚   â””â”€â”€ rl_config.yaml
â”œâ”€â”€ envs/                  # ä»¿çœŸç¯å¢ƒ
â”‚   â”œâ”€â”€ franka_env.py
â”‚   â””â”€â”€ trajectory_gen.py
â”œâ”€â”€ controllers/           # æ§åˆ¶å™¨
â”‚   â”œâ”€â”€ pid_controller.py
â”‚   â””â”€â”€ rl_pid_hybrid.py
â”œâ”€â”€ training/              # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_ddpg.py
â”‚   â””â”€â”€ callbacks.py
â”œâ”€â”€ evaluation/            # è¯„ä¼°è„šæœ¬
â”‚   â”œâ”€â”€ eval_scenarios.py
â”‚   â””â”€â”€ metrics.py
â”œâ”€â”€ visualization/         # å¯è§†åŒ–
â”‚   â””â”€â”€ plot_results.py
â””â”€â”€ experiments/           # å®éªŒè„šæœ¬
    â””â”€â”€ run_all_tests.py
```

---

### é˜¶æ®µ2: RL+PIDæ ¸å¿ƒç®—æ³•ç§»æ¤ï¼ˆWeek 2, 5-7å¤©ï¼‰

#### 2.1 ä¿å®ˆç‰ˆæœ¬å®ç°ï¼ˆæœ€é‡è¦ï¼ï¼‰

**æ ¸å¿ƒåŸåˆ™**: ä»MATLABçš„æ¿€è¿›é…ç½®å›é€€åˆ°å®‰å…¨é…ç½®

```python
# configs/rl_config.yaml
rl_params:
  # â­ è¡¥å¿å‚æ•°ï¼ˆä¿å®ˆç‰ˆï¼‰
  delta_scale_min: 0.5        # èµ·å§‹è¡¥å¿ï¼ˆåŸMATLABæ¿€è¿›ç‰ˆ=30.0ï¼‰
  delta_scale_max: 5.0        # æœ€å¤§è¡¥å¿ï¼ˆåŸMATLABæ¿€è¿›ç‰ˆ=50.0ï¼‰
  delta_tau_clip: 10.0        # é™åˆ¶Â±10Nm
  
  # â­ Warmupæœºåˆ¶ï¼ˆå¿…é¡»ä¿ç•™ï¼‰
  warmup_disable_steps: 100   # å‰100æ­¥çº¯PID
  warmup_ramp_steps: 500      # 100-600æ­¥æ¸è¿›å¢åŠ 
  
  # â­ å¥–åŠ±æƒé‡ï¼ˆå¹³è¡¡ç‰ˆï¼‰
  w_track: 20.0               # è·Ÿè¸ªå¥–åŠ±ï¼ˆåŸæ¿€è¿›ç‰ˆ=100.0ï¼‰
  w_vel: 0.001                # é€Ÿåº¦æƒ©ç½šï¼ˆåŸæ¿€è¿›ç‰ˆ=0ï¼‰
  w_action: 0.0001            # åŠ¨ä½œæƒ©ç½šï¼ˆåŸæ¿€è¿›ç‰ˆ=0ï¼‰
  w_smooth: 0.0001            # å¹³æ»‘æƒ©ç½š
  w_delta: 0.0001             # è¡¥å¿æƒ©ç½š
  
  # ç½‘ç»œç»“æ„
  actor_hidden: [512, 256, 128]
  critic_hidden: [256, 256, 512, 256]
  learning_rate_actor: 5e-4
  learning_rate_critic: 5e-4
  
  # è®­ç»ƒè¶…å‚æ•°
  buffer_size: 100000
  batch_size: 128
  gamma: 0.99
  tau: 0.01  # target network update rate
```

#### 2.2 ä»MATLABç§»æ¤çš„å…³é”®ä»£ç 

##### 2.2.1 æ¸è¿›å¼è¡¥å¿æœºåˆ¶

```python
# controllers/rl_pid_hybrid.py

class RLPIDHybrid:
    """
    ä»MATLAB RLHighLevelBlock_Toolbox.mç§»æ¤
    æ ¸å¿ƒï¼šPIDåŸºçº¿ + RLè¡¥å¿ï¼Œæ¸è¿›å¼å¯åŠ¨
    """
    def __init__(self, config):
        self.pid = PIDController(
            Kp=np.array([50, 50, 50, 50, 20, 10, 10]),  # å‚è€ƒMATLABé…ç½®
            Ki=np.array([0.5, 0.5, 0.5, 0.5, 0.2, 0.1, 0.1]),
            Kd=np.array([5, 5, 5, 5, 2, 1, 1])
        )
        
        self.rl_policy = None  # ç¨ååŠ è½½
        self.step_count = 0
        
        # ä»configåŠ è½½
        self.delta_scale_min = config['delta_scale_min']
        self.delta_scale_max = config['delta_scale_max']
        self.warmup_disable = config['warmup_disable_steps']
        self.warmup_ramp = config['warmup_ramp_steps']
        self.delta_clip = config['delta_tau_clip']
        
    def compute_control(self, q, qd, qref, training=False):
        """
        è®¡ç®—æ€»æ§åˆ¶åŠ›çŸ©
        å‚è€ƒMATLAB: RLHighLevelBlock_Toolbox.m ç¬¬213-241è¡Œ
        """
        # 1. PIDåŸºçº¿ï¼ˆå§‹ç»ˆå¼€å¯ï¼‰
        tau_pid = self.pid.compute(q, qd, qref)
        
        # 2. è®¡ç®—å½“å‰è¡¥å¿ç³»æ•°ï¼ˆâ­æ¸è¿›å¼ï¼‰
        if self.step_count < self.warmup_disable:
            # é˜¶æ®µ1: çº¯PIDï¼Œä¸è¡¥å¿
            delta_scale = 0.0
        elif self.step_count < self.warmup_disable + self.warmup_ramp:
            # é˜¶æ®µ2: çº¿æ€§å¢åŠ  0.5 â†’ 5.0
            progress = (self.step_count - self.warmup_disable) / self.warmup_ramp
            delta_scale = self.delta_scale_min + progress * (
                self.delta_scale_max - self.delta_scale_min
            )
        else:
            # é˜¶æ®µ3: å…¨åŠ›è¡¥å¿
            delta_scale = self.delta_scale_max
        
        # 3. RLè¡¥å¿
        if self.rl_policy is not None and delta_scale > 0:
            state = self._construct_state(q, qd, qref)
            raw_action = self.rl_policy.predict(state, deterministic=not training)
            
            # ç¼©æ”¾å¹¶è£å‰ª
            delta_tau = delta_scale * raw_action
            delta_tau = np.clip(delta_tau, -self.delta_clip, self.delta_clip)
        else:
            delta_tau = np.zeros(7)
        
        # 4. æ€»æ§åˆ¶
        tau_total = tau_pid + delta_tau
        
        self.step_count += 1
        
        return tau_total, tau_pid, delta_tau, delta_scale
    
    def _construct_state(self, q, qd, qref):
        """
        æ„é€ RLçŠ¶æ€å‘é‡: [q_err(7), qd(7)] = 14ç»´
        å‚è€ƒMATLABç¬¬175-185è¡Œ
        """
        q_err = qref - q
        return np.concatenate([q_err, qd])
```

##### 2.2.2 å¥–åŠ±å‡½æ•°

```python
# training/reward_function.py

def compute_reward(q, qd, qref, action, delta_tau, config):
    """
    ä»MATLAB RLHighLevelBlock_Toolbox.m ç¬¬250-272è¡Œç§»æ¤
    """
    # 1. è·Ÿè¸ªè¯¯å·®
    track_err = qref - q
    err_norm_sq = np.sum(track_err**2)
    err_norm = np.sqrt(err_norm_sq)
    
    # 2. ç´¯ç§¯è¯¯å·®ï¼ˆæŒ‡æ•°è¡°å‡ï¼Œå‚è€ƒMATLABç¬¬254-257è¡Œï¼‰
    if not hasattr(compute_reward, 'err_accum'):
        compute_reward.err_accum = np.zeros_like(track_err)
    compute_reward.err_accum = 0.95 * compute_reward.err_accum + track_err
    accum_penalty = np.sum(compute_reward.err_accum**2)
    
    # 3. åˆ†é¡¹å¥–åŠ±ï¼ˆä¿å®ˆæƒé‡ï¼‰
    w = config['reward_weights']
    
    r_track = -w['track'] * (err_norm_sq + 0.5*err_norm + 0.1*accum_penalty)
    r_vel = -w['vel'] * np.sum(qd**2)
    r_action = -w['action'] * np.sum(action**2)
    r_delta = -w['delta'] * np.sum(delta_tau**2)
    
    reward = r_track + r_vel + r_action + r_delta
    
    return reward, {
        'r_track': r_track,
        'r_vel': r_vel,
        'r_action': r_action,
        'r_delta': r_delta
    }
```

#### 2.3 PyBulletç¯å¢ƒå°è£…

```python
# envs/franka_env.py

import gym
import pybullet as p
import pybullet_data
import numpy as np

class FrankaRLPIDEnv(gym.Env):
    """
    Franka Panda + RL+PIDæ··åˆæ§åˆ¶ç¯å¢ƒ
    """
    def __init__(self, config, gui=False):
        super().__init__()
        
        # PyBulletåˆå§‹åŒ–
        self.gui = gui
        if gui:
            self.client = p.connect(p.GUI)
        else:
            self.client = p.connect(p.DIRECT)
        
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        p.setGravity(0, 0, -9.81)
        
        # åŠ è½½æœºå™¨äºº
        self.robot_id = p.loadURDF("franka_panda/panda.urdf", [0, 0, 0], useFixedBase=True)
        self.num_joints = 7
        
        # å…³èŠ‚ä¿¡æ¯
        self.joint_indices = list(range(self.num_joints))
        
        # æ§åˆ¶å™¨
        self.controller = RLPIDHybrid(config['rl_params'])
        
        # è½¨è¿¹ç”Ÿæˆå™¨
        self.traj_gen = TrajectoryGenerator(config['trajectory'])
        
        # Gymç©ºé—´
        self.observation_space = gym.spaces.Box(
            low=-np.inf, high=np.inf, shape=(14,), dtype=np.float32
        )
        self.action_space = gym.spaces.Box(
            low=-1, high=1, shape=(7,), dtype=np.float32
        )
        
        # æ—¶é—´æ­¥
        self.dt = 0.001  # 1kHzæ§åˆ¶é¢‘ç‡
        self.max_steps = 10000  # 10ç§’
        self.current_step = 0
        
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        # é‡ç½®æœºå™¨äººåˆ°åˆå§‹ä½ç½®
        init_q = np.array([0, -0.3, 0, -2.2, 0, 2.0, 0.79])
        for i, q in enumerate(init_q):
            p.resetJointState(self.robot_id, i, q)
        
        # é‡ç½®æ§åˆ¶å™¨
        self.controller.reset()
        self.current_step = 0
        self.traj_gen.reset()
        
        # è·å–åˆå§‹çŠ¶æ€
        q, qd = self._get_robot_state()
        qref = self.traj_gen.get_reference(0)
        
        state = self.controller._construct_state(q, qd, qref)
        return state
    
    def step(self, action):
        """
        æ‰§è¡Œä¸€æ­¥
        action: RLè¾“å‡ºçš„åŸå§‹åŠ¨ä½œ âˆˆ [-1, 1]^7
        """
        # è·å–å½“å‰çŠ¶æ€
        q, qd = self._get_robot_state()
        t = self.current_step * self.dt
        qref = self.traj_gen.get_reference(t)
        
        # è®©æ§åˆ¶å™¨å¤„ç†actionï¼ˆåŒ…æ‹¬PID+ç¼©æ”¾+è£å‰ªï¼‰
        self.controller.rl_policy = lambda s, **kwargs: action  # ä¸´æ—¶æ³¨å…¥
        tau_total, tau_pid, delta_tau, delta_scale = self.controller.compute_control(
            q, qd, qref, training=True
        )
        
        # åº”ç”¨åŠ›çŸ©
        p.setJointMotorControlArray(
            self.robot_id,
            self.joint_indices,
            p.TORQUE_CONTROL,
            forces=tau_total
        )
        
        # ä»¿çœŸä¸€æ­¥
        p.stepSimulation()
        
        # æ–°çŠ¶æ€
        q_new, qd_new = self._get_robot_state()
        qref_new = self.traj_gen.get_reference(t + self.dt)
        next_state = self.controller._construct_state(q_new, qd_new, qref_new)
        
        # è®¡ç®—å¥–åŠ±
        reward, reward_info = compute_reward(
            q_new, qd_new, qref_new, action, delta_tau, self.config
        )
        
        # æ£€æŸ¥ç»ˆæ­¢
        self.current_step += 1
        done = self.current_step >= self.max_steps
        
        # æ£€æŸ¥å‘æ•£ï¼ˆå®‰å…¨æœºåˆ¶ï¼‰
        if np.any(np.abs(q_new) > 3.0):  # å…³èŠ‚ä½ç½®è¶…é™
            reward -= 1000
            done = True
        
        info = {
            'tau_pid': tau_pid,
            'delta_tau': delta_tau,
            'delta_scale': delta_scale,
            'tracking_error': np.linalg.norm(qref_new - q_new),
            **reward_info
        }
        
        return next_state, reward, done, info
    
    def _get_robot_state(self):
        """è·å–å…³èŠ‚ä½ç½®å’Œé€Ÿåº¦"""
        joint_states = p.getJointStates(self.robot_id, self.joint_indices)
        q = np.array([s[0] for s in joint_states])
        qd = np.array([s[1] for s in joint_states])
        return q, qd
```

---

### é˜¶æ®µ3: æ¸è¿›å¼è®­ç»ƒï¼ˆWeek 2-3, 7-10å¤©ï¼‰

#### 3.1 è®­ç»ƒç­–ç•¥

**ç¬¬1å¤©**: éªŒè¯ç¯å¢ƒç¨³å®šæ€§
```bash
# çº¯PIDæµ‹è¯•ï¼ˆdelta_scale_max=0ï¼‰
python training/train_ddpg.py --config configs/test_pure_pid.yaml
```

**ç¬¬2-3å¤©**: å°è¡¥å¿è®­ç»ƒ
```yaml
# configs/stage1_small.yaml
delta_scale_max: 2.0
total_timesteps: 500000
```

**ç¬¬4-5å¤©**: ä¸­ç­‰è¡¥å¿
```yaml
# configs/stage2_medium.yaml
delta_scale_max: 5.0
total_timesteps: 1000000
```

**ç¬¬6-7å¤©**: å¤§è¡¥å¿ï¼ˆå¦‚æœå‰é¢ç¨³å®šï¼‰
```yaml
# configs/stage3_large.yaml
delta_scale_max: 10.0
total_timesteps: 1500000
```

#### 3.2 è®­ç»ƒè„šæœ¬

```python
# training/train_ddpg.py

from stable_baselines3 import DDPG
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.noise import NormalActionNoise

def train_rl_pid(config_path):
    # åŠ è½½é…ç½®
    config = load_config(config_path)
    
    # åˆ›å»ºç¯å¢ƒ
    train_env = FrankaRLPIDEnv(config, gui=False)
    eval_env = FrankaRLPIDEnv(config, gui=False)
    
    # åŠ¨ä½œå™ªå£°ï¼ˆæ¢ç´¢ï¼‰
    n_actions = train_env.action_space.shape[0]
    action_noise = NormalActionNoise(
        mean=np.zeros(n_actions),
        sigma=0.3 * np.ones(n_actions)  # 30%å™ªå£°
    )
    
    # åˆ›å»ºDDPGæ™ºèƒ½ä½“
    model = DDPG(
        "MlpPolicy",
        train_env,
        learning_rate=config['rl_params']['learning_rate_actor'],
        buffer_size=config['rl_params']['buffer_size'],
        batch_size=config['rl_params']['batch_size'],
        gamma=config['rl_params']['gamma'],
        tau=config['rl_params']['tau'],
        action_noise=action_noise,
        policy_kwargs={
            'net_arch': {
                'pi': config['rl_params']['actor_hidden'],
                'qf': config['rl_params']['critic_hidden']
            }
        },
        tensorboard_log="./logs/tensorboard/",
        verbose=1
    )
    
    # å›è°ƒå‡½æ•°
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path='./logs/best_model/',
        log_path='./logs/eval/',
        eval_freq=10000,
        n_eval_episodes=5,
        deterministic=True
    )
    
    checkpoint_callback = CheckpointCallback(
        save_freq=50000,
        save_path='./logs/checkpoints/',
        name_prefix='rl_pid_model'
    )
    
    # è®­ç»ƒ
    total_timesteps = config['training']['total_timesteps']
    model.learn(
        total_timesteps=total_timesteps,
        callback=[eval_callback, checkpoint_callback]
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(f"./models/rl_pid_final_{config['name']}")
    
    return model

if __name__ == "__main__":
    train_rl_pid("configs/stage1_small.yaml")
```

#### 3.3 è®­ç»ƒç›‘æ§

```python
# ä½¿ç”¨TensorBoardå®æ—¶ç›‘æ§
tensorboard --logdir=./logs/tensorboard/

# å…³é”®æŒ‡æ ‡ï¼š
# - rollout/ep_rew_mean: å¹³å‡å›åˆå¥–åŠ±ï¼ˆåº”è¯¥ä¸Šå‡ï¼‰
# - train/actor_loss: ActoræŸå¤±
# - train/critic_loss: CriticæŸå¤±
# - custom/tracking_error: è·Ÿè¸ªè¯¯å·®ï¼ˆåº”è¯¥ä¸‹é™ï¼‰
# - custom/delta_scale: è¡¥å¿ç³»æ•°ï¼ˆåº”è¯¥æ¸è¿›å¢åŠ åˆ°5.0ï¼‰
```

---

### é˜¶æ®µ4: å¤šåœºæ™¯æµ‹è¯•ï¼ˆWeek 3-4, 7-10å¤©ï¼‰

#### 4.1 è®¾è®¡25ç§æµ‹è¯•åœºæ™¯

```python
# evaluation/test_scenarios.py

TEST_SCENARIOS = {
    # === ç±»åˆ«1: ä¸åŒè½¨è¿¹é€Ÿåº¦ (5ç§) ===
    'circle_slow': {
        'trajectory': {'type': 'circle', 'radius': 0.1, 'speed': 0.1},
        'description': 'æ…¢é€Ÿåœ†å½¢è½¨è¿¹'
    },
    'circle_medium': {
        'trajectory': {'type': 'circle', 'radius': 0.1, 'speed': 0.3},
        'description': 'ä¸­é€Ÿåœ†å½¢è½¨è¿¹'
    },
    'circle_fast': {
        'trajectory': {'type': 'circle', 'radius': 0.1, 'speed': 0.5},
        'description': 'å¿«é€Ÿåœ†å½¢è½¨è¿¹'
    },
    'line_zigzag': {
        'trajectory': {'type': 'zigzag', 'amplitude': 0.2, 'frequency': 0.5},
        'description': 'ä¹‹å­—å½¢è½¨è¿¹'
    },
    'sine_wave': {
        'trajectory': {'type': 'sine', 'amplitude': 0.15, 'frequency': 0.3},
        'description': 'æ­£å¼¦æ³¢è½¨è¿¹'
    },
    
    # === ç±»åˆ«2: ä¸åŒè´Ÿè½½ (5ç§) ===
    'circle_load_0kg': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'payload_mass': 0.0,
        'description': 'æ— è´Ÿè½½'
    },
    'circle_load_1kg': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'payload_mass': 1.0,
        'description': '1kgè´Ÿè½½'
    },
    'circle_load_2kg': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'payload_mass': 2.0,
        'description': '2kgè´Ÿè½½'
    },
    'circle_load_3kg': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'payload_mass': 3.0,
        'description': '3kgè´Ÿè½½ï¼ˆæ¥è¿‘æé™ï¼‰'
    },
    'circle_load_variable': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'payload_mass': 'variable',  # è¿è¡Œä¸­å˜åŒ–
        'description': 'åŠ¨æ€å˜åŒ–è´Ÿè½½'
    },
    
    # === ç±»åˆ«3: æ¨¡å‹ä¸ç¡®å®šæ€§ (5ç§) ===
    'circle_mass_plus10': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'model_error': {'link_mass_scale': 1.1},
        'description': 'è´¨é‡é«˜ä¼°10%'
    },
    'circle_mass_plus20': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'model_error': {'link_mass_scale': 1.2},
        'description': 'è´¨é‡é«˜ä¼°20%'
    },
    'circle_mass_minus10': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'model_error': {'link_mass_scale': 0.9},
        'description': 'è´¨é‡ä½ä¼°10%'
    },
    'circle_mass_minus20': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'model_error': {'link_mass_scale': 0.8},
        'description': 'è´¨é‡ä½ä¼°20%'
    },
    'circle_inertia_error': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'model_error': {'inertia_scale': 1.3},
        'description': 'æƒ¯æ€§çŸ©è¯¯å·®30%'
    },
    
    # === ç±»åˆ«4: æ‘©æ“¦å’Œæ‰°åŠ¨ (5ç§) ===
    'circle_friction_2x': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'friction_scale': 2.0,
        'description': 'æ‘©æ“¦åŠ›2å€'
    },
    'circle_friction_05x': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'friction_scale': 0.5,
        'description': 'æ‘©æ“¦åŠ›å‡åŠ'
    },
    'circle_noise_low': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'sensor_noise_std': 0.001,
        'description': 'ä½å™ªå£°'
    },
    'circle_noise_high': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'sensor_noise_std': 0.01,
        'description': 'é«˜å™ªå£°'
    },
    'circle_external_force': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'external_force': {'magnitude': 5.0, 'frequency': 1.0},
        'description': 'å‘¨æœŸæ€§å¤–åŠ›æ‰°åŠ¨'
    },
    
    # === ç±»åˆ«5: ç»¼åˆæŒ‘æˆ˜ (5ç§) ===
    'fast_zigzag_load': {
        'trajectory': {'type': 'zigzag', 'speed': 0.5},
        'payload_mass': 2.0,
        'description': 'å¿«é€Ÿè½¨è¿¹+è´Ÿè½½'
    },
    'circle_all_errors': {
        'trajectory': {'type': 'circle', 'speed': 0.3},
        'model_error': {'link_mass_scale': 1.2},
        'sensor_noise_std': 0.005,
        'friction_scale': 1.5,
        'description': 'ç»¼åˆè¯¯å·®åœºæ™¯'
    },
    'sine_high_frequency': {
        'trajectory': {'type': 'sine', 'frequency': 2.0, 'amplitude': 0.1},
        'description': 'é«˜é¢‘æ­£å¼¦'
    },
    'figure_eight': {
        'trajectory': {'type': 'figure_eight', 'speed': 0.3},
        'description': '8å­—å½¢è½¨è¿¹'
    },
    'random_waypoints': {
        'trajectory': {'type': 'random_waypoints', 'n_points': 10},
        'description': 'éšæœºè·¯å¾„ç‚¹'
    }
}
```

#### 4.2 Monte Carloå®éªŒ

```python
# evaluation/monte_carlo.py

def run_monte_carlo(policy_path, scenario_name, n_trials=100):
    """
    å¯¹æ¯ä¸ªåœºæ™¯è·‘100æ¬¡ï¼Œè·å–ç»Ÿè®¡æ˜¾è‘—æ€§
    """
    results = []
    
    for seed in range(n_trials):
        # è®¾ç½®éšæœºç§å­
        np.random.seed(seed)
        torch.manual_seed(seed)
        
        # åˆ›å»ºç¯å¢ƒ
        scenario = TEST_SCENARIOS[scenario_name]
        env = FrankaRLPIDEnv(scenario, gui=False)
        
        # åŠ è½½ç­–ç•¥
        model = DDPG.load(policy_path)
        
        # è¿è¡Œä¸€æ¬¡å®Œæ•´episode
        obs = env.reset()
        done = False
        metrics = {
            'tracking_errors': [],
            'control_efforts': [],
            'delta_taus': []
        }
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            
            metrics['tracking_errors'].append(info['tracking_error'])
            metrics['control_efforts'].append(np.linalg.norm(info['tau_pid'] + info['delta_tau']))
            metrics['delta_taus'].append(np.linalg.norm(info['delta_tau']))
        
        # è®¡ç®—å•æ¬¡è¿è¡Œçš„ç»Ÿè®¡é‡
        result = {
            'rmse': np.sqrt(np.mean(np.array(metrics['tracking_errors'])**2)),
            'max_error': np.max(metrics['tracking_errors']),
            'mean_control_effort': np.mean(metrics['control_efforts']),
            'mean_delta_tau': np.mean(metrics['delta_taus']),
            'seed': seed
        }
        results.append(result)
    
    # ç»Ÿè®¡åˆ†æ
    rmse_values = [r['rmse'] for r in results]
    mean_rmse = np.mean(rmse_values)
    std_rmse = np.std(rmse_values)
    ci_95 = 1.96 * std_rmse / np.sqrt(n_trials)
    
    summary = {
        'scenario': scenario_name,
        'n_trials': n_trials,
        'mean_rmse': mean_rmse,
        'std_rmse': std_rmse,
        'ci_95': ci_95,
        'median_rmse': np.median(rmse_values),
        'all_results': results
    }
    
    return summary
```

#### 4.3 å¯¹æ¯”åŸºçº¿æ–¹æ³•

```python
# evaluation/baseline_methods.py

class PurePID:
    """åŸºçº¿1: çº¯PID"""
    def __init__(self, Kp, Ki, Kd):
        self.pid = PIDController(Kp, Ki, Kd)
    
    def compute_control(self, q, qd, qref):
        return self.pid.compute(q, qd, qref)

class AdaptivePID:
    """åŸºçº¿2: è‡ªé€‚åº”PIDï¼ˆMITè§„åˆ™ï¼‰"""
    def __init__(self, Kp_init, adaptation_gain):
        self.Kp = Kp_init
        self.gamma = adaptation_gain
    
    def compute_control(self, q, qd, qref):
        # MITè‡ªé€‚åº”è§„åˆ™
        e = qref - q
        self.Kp += self.gamma * e * e  # ç®€åŒ–ç‰ˆ
        return self.Kp * e

class ComputedTorqueControl:
    """åŸºçº¿3: åŸºäºæ¨¡å‹çš„Computed Torque + PID"""
    def __init__(self, robot_model, Kp, Kd):
        self.model = robot_model
        self.Kp = Kp
        self.Kd = Kd
    
    def compute_control(self, q, qd, qref, qd_ref, qdd_ref):
        # åé¦ˆçº¿æ€§åŒ–
        M = self.model.mass_matrix(q)
        C = self.model.coriolis(q, qd)
        G = self.model.gravity(q)
        
        # PDè¡¥å¿
        e = qref - q
        ed = qd_ref - qd
        a = qdd_ref + self.Kp * e + self.Kd * ed
        
        # è®¡ç®—åŠ›çŸ©
        tau = M @ a + C @ qd + G
        return tau

# å¯¹æ¯”å®éªŒ
def compare_all_methods():
    """
    å¯¹æ¯”4ç§æ–¹æ³•ï¼š
    1. Pure PID
    2. Adaptive PID
    3. Computed Torque Control
    4. RL+PID (Ours)
    """
    methods = {
        'PurePID': PurePID(...),
        'AdaptivePID': AdaptivePID(...),
        'ComputedTorque': ComputedTorqueControl(...),
        'RLPID_Ours': load_trained_model('models/rl_pid_final.zip')
    }
    
    results = {}
    
    for scenario_name in TEST_SCENARIOS:
        print(f"\n=== Testing {scenario_name} ===")
        results[scenario_name] = {}
        
        for method_name, method in methods.items():
            print(f"  Running {method_name}...")
            summary = run_monte_carlo_with_method(method, scenario_name, n_trials=100)
            results[scenario_name][method_name] = summary
    
    # ä¿å­˜ç»“æœ
    save_results(results, 'comparison_results.pkl')
    
    return results
```

---

### é˜¶æ®µ5: è®ºæ–‡æ’°å†™ä¸å›¾è¡¨ç”Ÿæˆï¼ˆWeek 5-6, 10-14å¤©ï¼‰

#### 5.1 å…³é”®å›¾è¡¨

```python
# visualization/paper_figures.py

def generate_paper_figures(results):
    """
    ç”Ÿæˆè®ºæ–‡æ‰€éœ€çš„æ‰€æœ‰å›¾è¡¨
    """
    
    # å›¾1: å…¸å‹åœºæ™¯çš„è½¨è¿¹è·Ÿè¸ªå¯¹æ¯”
    fig1 = plot_trajectory_comparison(
        scenario='circle_medium',
        methods=['PurePID', 'RLPID_Ours']
    )
    fig1.savefig('figures/fig1_trajectory_comparison.pdf', dpi=300)
    
    # å›¾2: è¯¯å·®éšæ—¶é—´å˜åŒ–
    fig2 = plot_error_evolution(
        scenario='circle_medium',
        methods=['PurePID', 'AdaptivePID', 'ComputedTorque', 'RLPID_Ours']
    )
    fig2.savefig('figures/fig2_error_evolution.pdf', dpi=300)
    
    # å›¾3: RLè¡¥å¿åŠ›çŸ©åˆ†æ
    fig3 = plot_delta_tau_analysis(
        scenario='circle_medium',
        joint_idx=[0, 3, 6]  # æ˜¾ç¤ºå…³èŠ‚1, 4, 7
    )
    fig3.savefig('figures/fig3_delta_tau_analysis.pdf', dpi=300)
    
    # å›¾4: ç®±çº¿å›¾ - Monte Carloç»Ÿè®¡
    fig4 = plot_boxplot_comparison(
        scenarios=['circle_slow', 'circle_medium', 'circle_fast'],
        methods=['PurePID', 'RLPID_Ours']
    )
    fig4.savefig('figures/fig4_boxplot_comparison.pdf', dpi=300)
    
    # å›¾5: çƒ­å›¾ - 25åœºæ™¯å…¨é¢å¯¹æ¯”
    fig5 = plot_heatmap_all_scenarios(results)
    fig5.savefig('figures/fig5_heatmap_all_scenarios.pdf', dpi=300)
    
    # å›¾6: è®­ç»ƒæ›²çº¿
    fig6 = plot_training_curves('logs/tensorboard/')
    fig6.savefig('figures/fig6_training_curves.pdf', dpi=300)
    
    # å›¾7: æ¶ˆèå®éªŒ
    fig7 = plot_ablation_study({
        'No RL': 'models/pure_pid.zip',
        'RL w/o warmup': 'models/rl_no_warmup.zip',
        'RL w/ warmup (Ours)': 'models/rl_pid_final.zip'
    })
    fig7.savefig('figures/fig7_ablation_study.pdf', dpi=300)
    
    # å›¾8: é²æ£’æ€§åˆ†æï¼ˆä¸åŒæ‰°åŠ¨ä¸‹çš„æ€§èƒ½ï¼‰
    fig8 = plot_robustness_analysis(
        scenarios=['circle_noise_low', 'circle_noise_high', 
                   'circle_friction_2x', 'circle_external_force']
    )
    fig8.savefig('figures/fig8_robustness_analysis.pdf', dpi=300)

#### 5.2 ç»Ÿè®¡è¡¨æ ¼

```python
# evaluation/generate_tables.py

def generate_latex_table(results):
    """
    ç”ŸæˆLaTeXæ ¼å¼çš„å¯¹æ¯”è¡¨æ ¼
    """
    
    # è¡¨1: ä¸»è¦åœºæ™¯çš„RMSEå¯¹æ¯”ï¼ˆå‡å€¼Â±æ ‡å‡†å·®ï¼‰
    table1 = f"""
\\begin{table}[ht]
\\centering
\\caption{{Tracking RMSE Comparison (rad, mean Â± std, n=100)}}
\\label{{tab:rmse_comparison}}
\\begin{{tabular}}{{lcccc}}
\\hline
Scenario & Pure PID & Adaptive PID & Computed Torque & RL+PID (Ours) \\\\
\\hline
"""
    
    key_scenarios = ['circle_slow', 'circle_medium', 'circle_fast', 
                     'circle_load_2kg', 'circle_mass_plus20']
    
    for scenario in key_scenarios:
        row = f"{scenario}"
        for method in ['PurePID', 'AdaptivePID', 'ComputedTorque', 'RLPID_Ours']:
            mean = results[scenario][method]['mean_rmse']
            std = results[scenario][method]['std_rmse']
            row += f" & {mean:.4f}$\\pm${std:.4f}"
        
        # é«˜äº®æœ€å¥½ç»“æœ
        row += " \\\\\n"
        table1 += row
    
    table1 += """\\hline
\\end{tabular}
\\end{table}
"""
    
    with open('tables/table1_rmse_comparison.tex', 'w') as f:
        f.write(table1)
    
    # è¡¨2: æ”¹è¿›ç™¾åˆ†æ¯”
    # è¡¨3: è®¡ç®—æ•ˆç‡å¯¹æ¯”
    # ...
```

#### 5.3 è®ºæ–‡å¤§çº²ç”Ÿæˆ

```markdown
# è®ºæ–‡å¤§çº²ï¼ˆControl Engineering Practiceï¼‰

## Title
"RL-Enhanced PID Control for Robotic Manipulators: A Progressive Compensation Approach"

## Abstract (150-200 words)
- Background: PIDæ§åˆ¶çš„å±€é™æ€§
- Motivation: RLå¯ä»¥å­¦ä¹ è¡¥å¿æ¨¡å‹è¯¯å·®
- Method: æ¸è¿›å¼RLè¡¥å¿ + PIDåŸºçº¿
- Results: 25åœºæ™¯ï¼Œå¹³å‡RMSEé™ä½43%
- Contribution: å®ç”¨ä¸”ç¨³å®šçš„æ··åˆæ§åˆ¶æ–¹æ³•

## I. Introduction (2é¡µ)
1.1 Motivation
- æœºå™¨äººæ§åˆ¶ä¸­çš„æ¨¡å‹ä¸ç¡®å®šæ€§é—®é¢˜
- PIDç®€å•ä½†ç²¾åº¦å—é™
- RLå¼ºå¤§ä½†ç¨³å®šæ€§å·®

1.2 Related Work
- ä¼ ç»Ÿè‡ªé€‚åº”æ§åˆ¶
- åŸºäºå­¦ä¹ çš„æ§åˆ¶
- RL+ä¼ ç»Ÿæ§åˆ¶æ··åˆæ–¹æ³•

1.3 Contributions
- âœ… æ¸è¿›å¼è¡¥å¿æœºåˆ¶ï¼ˆè§£å†³RLåˆæœŸä¸ç¨³å®šï¼‰
- âœ… 25åœºæ™¯ç³»ç»ŸåŒ–æµ‹è¯•ï¼ˆé²æ£’æ€§éªŒè¯ï¼‰
- âœ… Monte Carloç»Ÿè®¡åˆ†æï¼ˆ100æ¬¡Ã—25åœºæ™¯ï¼‰

## II. Problem Formulation (1é¡µ)
2.1 Robot Dynamics
2.2 Control Objective
2.3 Challenges

## III. Methodology (3é¡µ)
3.1 System Architecture
- PIDåŸºçº¿æ§åˆ¶å™¨
- RLè¡¥å¿æ¨¡å—
- æ¸è¿›å¼ç¼©æ”¾æœºåˆ¶

3.2 RL Training
- çŠ¶æ€ç©ºé—´è®¾è®¡
- å¥–åŠ±å‡½æ•°è®¾è®¡
- DDPGç®—æ³•

3.3 Progressive Compensation Strategy
- Warmupé˜¶æ®µ
- Ramp-upé˜¶æ®µ
- å…¨è¡¥å¿é˜¶æ®µ

## IV. Experimental Setup (1.5é¡µ)
4.1 Simulation Platform
- PyBullet + Franka Panda
- ç‰©ç†å‚æ•°

4.2 Test Scenarios (25ç§)
- è¡¨æ ¼åˆ—å‡º5ç±»åœºæ™¯

4.3 Baseline Methods
- Pure PID
- Adaptive PID
- Computed Torque Control

4.4 Evaluation Metrics
- RMSE, Max Error
- Control Effort
- Settling Time

## V. Results (3é¡µ)
5.1 Training Performance
- è®­ç»ƒæ›²çº¿
- æ”¶æ•›é€Ÿåº¦

5.2 Tracking Performance
- å…¸å‹åœºæ™¯å¯¹æ¯”
- è¯¯å·®åˆ†æ

5.3 Comprehensive Comparison
- 25åœºæ™¯çƒ­å›¾
- ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ

5.4 Robustness Analysis
- ä¸åŒæ‰°åŠ¨ä¸‹çš„è¡¨ç°
- é²æ£’æ€§æŒ‡æ ‡

5.5 Ablation Study
- Warmupçš„ä½œç”¨
- è¡¥å¿ç³»æ•°çš„å½±å“

## VI. Discussion (1é¡µ)
- æ–¹æ³•çš„ä¼˜åŠ¿ä¸å±€é™
- å®é™…éƒ¨ç½²è€ƒè™‘
- æœªæ¥å·¥ä½œ

## VII. Conclusion (0.5é¡µ)
```

---

## ğŸš¨ å…³é”®æˆåŠŸå› ç´ 

### 1. ç¨³å®šæ€§ä¼˜å…ˆ
```
è°ƒå‚ä¼˜å…ˆçº§ï¼š
1. ç³»ç»Ÿç¨³å®šï¼ˆä¸å‘æ•£ï¼‰ >>> 2. æ€§èƒ½æå‡ >>> 3. è®­ç»ƒé€Ÿåº¦
```

### 2. å¯¹æ¯”åŸºçº¿è¦å…¬å¹³
```python
# ç¡®ä¿æ‰€æœ‰æ–¹æ³•ä½¿ç”¨ç›¸åŒçš„ï¼š
- è½¨è¿¹éš¾åº¦
- åˆå§‹æ¡ä»¶
- éšæœºç§å­
- è¯„ä¼°æŒ‡æ ‡
```

### 3. ç»Ÿè®¡æ˜¾è‘—æ€§
```python
# æ¯ä¸ªåœºæ™¯è·‘100æ¬¡ï¼ŒæŠ¥å‘Šï¼š
- å‡å€¼ Â± æ ‡å‡†å·®
- 95%ç½®ä¿¡åŒºé—´
- t-test p-value < 0.05
```

### 4. å¯å¤ç°æ€§
```bash
# æä¾›ï¼š
- requirements.txtï¼ˆä¾èµ–ç‰ˆæœ¬ï¼‰
- é…ç½®æ–‡ä»¶ï¼ˆæ‰€æœ‰è¶…å‚æ•°ï¼‰
- éšæœºç§å­
- Dockeré•œåƒï¼ˆå¯é€‰ï¼‰
```

---

## ğŸ“Š é¢„æœŸæˆæœ

### æœ€å°å¯è¡Œç»“æœï¼ˆä¿åº•ï¼‰
- âœ… 5ä¸ªæ ¸å¿ƒåœºæ™¯
- âœ… RMSEé™ä½ > 30%
- âœ… ç³»ç»Ÿç¨³å®šï¼Œä¸å‘æ•£
- âœ… å¯æŠ•Tier 3æœŸåˆŠ

### ç†æƒ³ç»“æœï¼ˆå†²åˆºTier 2ï¼‰
- âœ… 25ä¸ªåœºæ™¯å…¨è¦†ç›–
- âœ… RMSEå¹³å‡é™ä½ > 40%
- âœ… 3ä¸ªåŸºçº¿æ–¹æ³•å¯¹æ¯”
- âœ… Monte Carlo 100æ¬¡ç»Ÿè®¡
- âœ… å¯æŠ•Control Engineering Practice

### é¡¶çº§ç»“æœï¼ˆå†²åˆºTier 1ï¼Œéœ€è¦é¢å¤–å·¥ä½œï¼‰
- âœ… ç®€å•çš„ç¨³å®šæ€§åˆ†æï¼ˆLyapunovï¼‰
- âœ… GazeboéªŒè¯
- âœ… å®ç‰©å®éªŒï¼ˆå¦‚æœæœ‰æ¡ä»¶ï¼‰
- âœ… å¯æŠ•IEEE RAL

---

## ğŸ“… è¯¦ç»†æ—¶é—´è¡¨

| å‘¨æ¬¡ | ä»»åŠ¡ | å¯äº¤ä»˜æˆæœ | å·¥ä½œé‡ï¼ˆå¤©ï¼‰ |
|-----|------|-----------|------------|
| Week 1 | ç¯å¢ƒæ­å»º | PyBulletæµ‹è¯•é€šè¿‡ | 3-5å¤© |
| Week 2 | ç®—æ³•ç§»æ¤ | çº¯PIDç¨³å®šè¿è¡Œ | 5-7å¤© |
| Week 3 | æ¸è¿›è®­ç»ƒ | æ¨¡å‹æ”¶æ•›ï¼Œdelta_scale=5.0 | 7-10å¤© |
| Week 4 | å¤šåœºæ™¯æµ‹è¯• | 25åœºæ™¯è·‘é€š | 7-10å¤© |
| Week 5 | Monte Carlo | ç»Ÿè®¡ç»“æœ | 5-7å¤© |
| Week 6 | è®ºæ–‡æ’°å†™ | åˆç¨¿ | 10-14å¤© |
| **æ€»è®¡** | **6-8å‘¨** | **å®Œæ•´è®ºæ–‡** | **37-53å¤©** |

---

## ğŸ¯ ç«‹å³å¼€å§‹ï¼šç¬¬ä¸€æ­¥

ç°åœ¨å°±å¼€å§‹é˜¶æ®µ1ï¼æˆ‘å°†ä¸ºæ‚¨ï¼š

1. âœ… åˆ›å»ºå®Œæ•´çš„é¡¹ç›®ç»“æ„
2. âœ… ç”Ÿæˆç¯å¢ƒæµ‹è¯•è„šæœ¬
3. âœ… é…ç½®æ–‡ä»¶æ¨¡æ¿
4. âœ… è®­ç»ƒè„šæœ¬éª¨æ¶

**å‡†å¤‡å¼€å§‹äº†å—ï¼Ÿ**å›å¤"å¼€å§‹"ï¼Œæˆ‘å°†ç«‹å³åˆ›å»ºæ‰€æœ‰æ–‡ä»¶ï¼

